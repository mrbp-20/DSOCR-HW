2026-01-10 19:56:58 - train_lora - INFO - ================================================================================
2026-01-10 19:56:58 - train_lora - INFO - Запуск обучения LoRA
2026-01-10 19:56:58 - train_lora - INFO - ================================================================================
2026-01-10 19:56:58 - train_lora - INFO - Конфигурация загружена из configs\training_config.yaml
2026-01-10 19:56:58 - train_lora - INFO - 
================================================================================
2026-01-10 19:56:58 - train_lora - INFO - Шаг 1/5: Загрузка модели и процессора...
2026-01-10 19:56:58 - train_lora - INFO - ================================================================================
2026-01-10 19:56:58 - train_lora - INFO - Загрузка модели и процессора...
2026-01-10 19:57:00 - train_lora - INFO - Процессор загружен: deepseek-ai/DeepSeek-OCR (revision: 9f30c71f441d010e5429c532364a86705536c53a)
2026-01-10 19:57:09 - train_lora - INFO - Модель загружена: deepseek-ai/DeepSeek-OCR (revision: 9f30c71f441d010e5429c532364a86705536c53a)
2026-01-10 19:57:09 - train_lora - INFO - OK Шаг 1 завершён
2026-01-10 19:57:09 - train_lora - INFO - 
================================================================================
2026-01-10 19:57:09 - train_lora - INFO - Шаг 2/5: Настройка LoRA...
2026-01-10 19:57:09 - train_lora - INFO - ================================================================================
2026-01-10 19:57:09 - train_lora - INFO - Настройка LoRA...
2026-01-10 19:57:09 - train_lora - INFO - Оборачиваем модель для совместимости с PEFT...
2026-01-10 19:57:16 - train_lora - INFO - LoRA настроен успешно
2026-01-10 19:57:16 - train_lora - INFO - OK Шаг 2 завершён
2026-01-10 19:57:16 - train_lora - INFO - 
================================================================================
2026-01-10 19:57:16 - train_lora - INFO - Шаг 3/5: Подготовка датасетов...
2026-01-10 19:57:16 - train_lora - INFO - ================================================================================
2026-01-10 19:57:16 - train_lora - INFO - Подготовка датасетов...
2026-01-10 19:57:16 - train_lora - INFO - Загружено train: 3, val: 1 образцов
2026-01-10 19:57:16 - train_lora - INFO - Датасеты подготовлены
2026-01-10 19:57:16 - train_lora - INFO - OK Шаг 3 завершён
2026-01-10 19:57:16 - train_lora - INFO - 
================================================================================
2026-01-10 19:57:16 - train_lora - INFO - Шаг 4/5: Создание Trainer...
2026-01-10 19:57:16 - train_lora - INFO - ================================================================================
2026-01-10 19:57:16 - train_lora - INFO - Создание Trainer...
2026-01-10 19:57:17 - train_lora - INFO - Trainer создан
2026-01-10 19:57:17 - train_lora - INFO - OK Шаг 4 завершён
2026-01-10 19:57:17 - train_lora - INFO - 
================================================================================
2026-01-10 19:57:17 - train_lora - INFO - Шаг 5/5: ЗАПУСК ОБУЧЕНИЯ!
2026-01-10 19:57:17 - train_lora - INFO - ================================================================================
2026-01-10 19:57:17 - train_lora - INFO - ================================================================================
2026-01-10 19:57:17 - train_lora - INFO - Начало обучения
2026-01-10 19:57:17 - train_lora - INFO - ================================================================================
2026-01-10 19:57:25 - train_lora - ERROR - Ошибка обучения: Attempting to unscale FP16 gradients.
Traceback (most recent call last):
  File "C:\DSOCR-HW\utils\trainer.py", line 508, in train
    self.trainer.train()
  File "C:\DSOCR-HW\venv\Lib\site-packages\transformers\trainer.py", line 2052, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\DSOCR-HW\venv\Lib\site-packages\transformers\trainer.py", line 2434, in _inner_training_loop
    _grad_norm = self.accelerator.clip_grad_norm_(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\DSOCR-HW\venv\Lib\site-packages\accelerate\accelerator.py", line 3008, in clip_grad_norm_
    self.unscale_gradients()
  File "C:\DSOCR-HW\venv\Lib\site-packages\accelerate\accelerator.py", line 2946, in unscale_gradients
    self.scaler.unscale_(opt)
  File "C:\DSOCR-HW\venv\Lib\site-packages\torch\amp\grad_scaler.py", line 358, in unscale_
    optimizer_state["found_inf_per_device"] = self._unscale_grads_(
                                              ^^^^^^^^^^^^^^^^^^^^^
  File "C:\DSOCR-HW\venv\Lib\site-packages\torch\amp\grad_scaler.py", line 275, in _unscale_grads_
    raise ValueError("Attempting to unscale FP16 gradients.")
ValueError: Attempting to unscale FP16 gradients.
2026-01-10 19:57:25 - train_lora - ERROR - Ошибка: Attempting to unscale FP16 gradients.
Traceback (most recent call last):
  File "C:\DSOCR-HW\scripts\train_lora.py", line 83, in main
    trainer.train()
  File "C:\DSOCR-HW\utils\trainer.py", line 508, in train
    self.trainer.train()
  File "C:\DSOCR-HW\venv\Lib\site-packages\transformers\trainer.py", line 2052, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "C:\DSOCR-HW\venv\Lib\site-packages\transformers\trainer.py", line 2434, in _inner_training_loop
    _grad_norm = self.accelerator.clip_grad_norm_(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\DSOCR-HW\venv\Lib\site-packages\accelerate\accelerator.py", line 3008, in clip_grad_norm_
    self.unscale_gradients()
  File "C:\DSOCR-HW\venv\Lib\site-packages\accelerate\accelerator.py", line 2946, in unscale_gradients
    self.scaler.unscale_(opt)
  File "C:\DSOCR-HW\venv\Lib\site-packages\torch\amp\grad_scaler.py", line 358, in unscale_
    optimizer_state["found_inf_per_device"] = self._unscale_grads_(
                                              ^^^^^^^^^^^^^^^^^^^^^
  File "C:\DSOCR-HW\venv\Lib\site-packages\torch\amp\grad_scaler.py", line 275, in _unscale_grads_
    raise ValueError("Attempting to unscale FP16 gradients.")
ValueError: Attempting to unscale FP16 gradients.
