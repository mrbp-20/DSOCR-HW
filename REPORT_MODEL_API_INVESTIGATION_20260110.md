# üî¨ –û–¢–ß–Å–¢: –ò–°–°–õ–ï–î–û–í–ê–ù–ò–ï API DeepSeek-OCR

**–î–∞—Ç–∞:** 2026-01-10  
**–ò—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å:** –ù–∏–∫–æ–ª–∞–π (Senior ML Engineer)  
**–ó–∞–¥–∞–Ω–∏–µ:** TASK_INVESTIGATE_MODEL_API_20260110.md

---

## üéØ –ö–õ–Æ–ß–ï–í–û–ï –û–¢–ö–†–´–¢–ò–ï

**DeepSeek-OCR –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ—Ç `pixel_values`!**

–ú–æ–¥–µ–ª—å –ø—Ä–∏–Ω–∏–º–∞–µ—Ç:
- **`images`** (–Ω–µ `pixel_values`!)
- **`images_seq_mask`**
- **`images_spatial_crop`**

---

## 1. –ü–ê–†–ê–ú–ï–¢–†–´ model.forward()

### –ù–∞–π–¥–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–∏–∑ `inspect.signature`):

```python
model.forward(
    input_ids: torch.LongTensor = None,
    attention_mask: Optional[torch.Tensor] = None,
    position_ids: Optional[torch.LongTensor] = None,
    past_key_values: Optional[List[torch.FloatTensor]] = None,
    inputs_embeds: Optional[torch.FloatTensor] = None,
    labels: Optional[torch.LongTensor] = None,
    use_cache: Optional[bool] = None,
    output_attentions: Optional[bool] = None,
    output_hidden_states: Optional[bool] = None,
    images: Optional[torch.FloatTensor] = None,  # ‚Üê –ö–õ–Æ–ß–ï–í–û–ô –ü–ê–†–ê–ú–ï–¢–†!
    images_seq_mask: Optional[torch.FloatTensor] = None,  # ‚Üê –¢–†–ï–ë–£–ï–¢–°–Ø?
    images_spatial_crop: Optional[torch.FloatTensor] = None,  # ‚Üê –¢–†–ï–ë–£–ï–¢–°–Ø?
    return_dict: Optional[bool] = None
)
```

### –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ vs –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ

**–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ (–¥–ª—è training):**
- `input_ids` - —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã
- `labels` - –º–µ—Ç–∫–∏ –¥–ª—è loss
- `images` - –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (—Ñ–æ—Ä–º–∞—Ç?)

**–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ:**
- `attention_mask` - –º–∞—Å–∫–∞ –≤–Ω–∏–º–∞–Ω–∏—è
- `images_seq_mask` - –º–∞—Å–∫–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π?
- `images_spatial_crop` - –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–µ crops –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π?
- –û—Å—Ç–∞–ª—å–Ω—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã

### –§–æ—Ä–º–∞—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤

- **`images`**: `Optional[torch.FloatTensor]` - —Ñ–æ—Ä–º–∞—Ç –Ω–µ—è—Å–µ–Ω, –Ω—É–∂–Ω–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å
- **`images_seq_mask`**: `Optional[torch.FloatTensor]` - –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ –Ω–µ—è—Å–Ω–æ
- **`images_spatial_crop`**: `Optional[torch.FloatTensor]` - –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ –Ω–µ—è—Å–Ω–æ

---

## 2. –í–´–í–û–î processor()

### –ü—Ä–æ–±–ª–µ–º–∞

**`AutoProcessor.from_pretrained()` –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–æ–ª—å–∫–æ `LlamaTokenizerFast`!**

–≠—Ç–æ **–ù–ï Vision Processor** - –æ–Ω –Ω–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.

### –¢–µ—Å—Ç—ã processor()

#### –¢–µ—Å—Ç 1: `processor(images=..., text=...)`

```python
inputs = processor(images=image, text=text, return_tensors="pt")
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
```
TypeError: PreTrainedTokenizerFast._batch_encode_plus() got an unexpected keyword argument 'images'
```

**–í—ã–≤–æ–¥:** Processor (tokenizer) –Ω–µ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç `images`!

#### –¢–µ—Å—Ç 2: `processor(images=...)` –±–µ–∑ text

```python
inputs = processor(images=image, return_tensors="pt")
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
```
ValueError: You need to specify either `text` or `text_target`.
```

**–í—ã–≤–æ–¥:** Processor —Ç—Ä–µ–±—É–µ—Ç `text` –∏ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å `images`.

### –ö–ª—é—á–∏ –≤ outputs processor()

**–î–ª—è —Ç–µ–∫—Å—Ç–∞:**
- `input_ids`: `torch.LongTensor` - —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
- `attention_mask`: `torch.Tensor` - –º–∞—Å–∫–∞ –≤–Ω–∏–º–∞–Ω–∏—è

**–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è:** Processor –ù–ï –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è!

---

## 3. –ü–†–ê–í–ò–õ–¨–ù–´–ô –§–û–†–ú–ê–¢ BATCH

### –¢–µ–∫—É—â–∞—è –ø—Ä–æ–±–ª–µ–º–∞

–ù–∞—à `data_collator` —Å–æ–∑–¥–∞—ë—Ç batch —Å:
- `pixel_values` - ‚ùå –ù–ï–ü–†–ê–í–ò–õ–¨–ù–û!
- `input_ids`
- `attention_mask`
- `labels`

–ù–æ –º–æ–¥–µ–ª—å –æ–∂–∏–¥–∞–µ—Ç:
- `images` - ‚ùì –ö–∞–∫ —Å–æ–∑–¥–∞—Ç—å?
- `images_seq_mask` - ‚ùì –ù—É–∂–µ–Ω –ª–∏?
- `images_spatial_crop` - ‚ùì –ù—É–∂–µ–Ω –ª–∏?
- `input_ids`
- `attention_mask`
- `labels`

### –ì–∏–ø–æ—Ç–µ–∑—ã

1. **`images`** –º–æ–∂–µ—Ç –±—ã—Ç—å:
   - PIL Image (–ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω—ã–π –≤ —Ç–µ–Ω–∑–æ—Ä)?
   - –¢–µ–Ω–∑–æ—Ä –ø–æ—Å–ª–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–µ—Ä–µ–∑ Vision Processor?
   - –ö–∞–∫–æ–π-—Ç–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç?

2. **`images_seq_mask`** –º–æ–∂–µ—Ç –±—ã—Ç—å:
   - –ú–∞—Å–∫–∞ –¥–ª—è batch'–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π?
   - –£–∫–∞–∑—ã–≤–∞–µ—Ç, –∫–∞–∫–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤–∞–ª–∏–¥–Ω—ã?

3. **`images_spatial_crop`** –º–æ–∂–µ—Ç –±—ã—Ç—å:
   - –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω—ã–µ crops –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è?
   - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è multi-scale –æ–±—Ä–∞–±–æ—Ç–∫–∏?

---

## 4. DATA COLLATOR

### –¢–µ–∫—É—â–∏–π data_collator

**–§–∞–π–ª:** `utils/trainer.py`, –º–µ—Ç–æ–¥ `_data_collator()`

**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç:**
- –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —á–µ—Ä–µ–∑ PIL
- –ü—Ä–∏–º–µ–Ω—è–µ—Ç `torchvision.transforms` (Resize + ToTensor + Normalize)
- –°–æ–∑–¥–∞—ë—Ç `pixel_values`

**–ü—Ä–æ–±–ª–µ–º–∞:**
- –°–æ–∑–¥–∞—ë—Ç `pixel_values`, –Ω–æ –º–æ–¥–µ–ª—å –æ–∂–∏–¥–∞–µ—Ç `images`
- –ù–µ —Å–æ–∑–¥–∞—ë—Ç `images_seq_mask` –∏ `images_spatial_crop`

### –ù—É–∂–µ–Ω –ª–∏ –∫–∞—Å—Ç–æ–º–Ω—ã–π DataCollator?

**–î–ê!** –¢–µ–∫—É—â–∏–π data_collator –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π.

**–ß—Ç–æ –Ω—É–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å:**

1. **–ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞—Ç—å `pixel_values` ‚Üí `images`:**
   ```python
   batch = {
       'images': pixel_values,  # –≤–º–µ—Å—Ç–æ pixel_values
       'input_ids': ...,
       'attention_mask': ...,
       'labels': ...
   }
   ```

2. **–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å, –Ω—É–∂–Ω—ã –ª–∏ `images_seq_mask` –∏ `images_spatial_crop`:**
   - –ï—Å–ª–∏ –æ–Ω–∏ –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ (`Optional`), –≤–æ–∑–º–æ–∂–Ω–æ, –º–æ–∂–Ω–æ –Ω–µ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å?
   - –ò–ª–∏ –Ω—É–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å –∑–∞–≥–ª—É—à–∫–∏?

3. **–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å —Ñ–æ—Ä–º–∞—Ç `images`:**
   - –î–æ–ª–∂–µ–Ω –ª–∏ —ç—Ç–æ –±—ã—Ç—å —Ç–µ–Ω–∑–æ—Ä –ø–æ—Å–ª–µ torchvision transforms?
   - –ò–ª–∏ –Ω—É–∂–µ–Ω –¥—Ä—É–≥–æ–π —Ñ–æ—Ä–º–∞—Ç –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏?

---

## 5. –ü–†–ò–ú–ï–†–´ –ò–ó –û–§–ò–¶. –†–ï–ü–û

### –ù–∞–π–¥–µ–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (–∏–∑ –∑–∞–¥–∞–Ω–∏—è)

1. **HuggingFace Model Card:**
   - https://huggingface.co/deepseek-ai/DeepSeek-OCR
   - –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å `multi_modal_data`
   
2. **GitHub Repository:**
   - https://github.com/deepseek-ai/DeepSeek-OCR
   - –ù—É–∂–Ω–æ –∏–∑—É—á–∏—Ç—å –ø—Ä–∏–º–µ—Ä—ã –æ–±—É—á–µ–Ω–∏—è

3. **Technical Report:**
   - https://pkulium.github.io/DeepOCR_website/
   - –£–ø–æ–º–∏–Ω–∞–Ω–∏–µ `images_crop` –∏ `images_spatial_crop`

### –°—Ç–∞—Ç—É—Å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è

**–ï—â—ë –Ω–µ –ø—Ä–æ–≤–µ–¥–µ–Ω–æ:**
- ‚ùå –ü–æ–∏—Å–∫ –ø—Ä–∏–º–µ—Ä–æ–≤ –æ–±—É—á–µ–Ω–∏—è –≤ GitHub
- ‚ùå –ò–∑—É—á–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ HuggingFace
- ‚ùå –ò–∑—É—á–µ–Ω–∏–µ Technical Report

**–°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:**
1. –ò–∑—É—á–∏—Ç—å GitHub —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π DeepSeek-OCR
2. –ù–∞–π—Ç–∏ –ø—Ä–∏–º–µ—Ä—ã fine-tuning/training
3. –ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å, –∫–∞–∫ –æ–Ω–∏ —Å–æ–∑–¥–∞—é—Ç batch –¥–ª—è –æ–±—É—á–µ–Ω–∏—è

---

## 6. –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò

### –ß—Ç–æ –Ω—É–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å –≤ –Ω–∞—à–µ–º –∫–æ–¥–µ

#### 1. –í data_collator (–ü–†–ò–û–†–ò–¢–ï–¢ 1 - –ö–†–ò–¢–ò–ß–ù–û)

**–§–∞–π–ª:** `utils/trainer.py`, –º–µ—Ç–æ–¥ `_data_collator()`

**–ò–∑–º–µ–Ω–µ–Ω–∏—è:**

```python
# –ë–´–õ–û:
batch = {
    'pixel_values': pixel_values,
    'input_ids': text_inputs['input_ids'],
    'attention_mask': text_inputs['attention_mask'],
    'labels': text_inputs['input_ids'].clone()
}

# –î–û–õ–ñ–ù–û –ë–´–¢–¨:
batch = {
    'images': pixel_values,  # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞—Ç—å pixel_values ‚Üí images
    'input_ids': text_inputs['input_ids'],
    'attention_mask': text_inputs['attention_mask'],
    'labels': text_inputs['input_ids'].clone()
    # images_seq_mask –∏ images_spatial_crop - –ø–æ–∫–∞ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º (Optional)
}
```

#### 2. –í model_wrapper.py (–ü–†–ò–û–†–ò–¢–ï–¢ 2)

**–§–∞–π–ª:** `utils/model_wrapper.py`, –º–µ—Ç–æ–¥ `forward()`

**–ò–∑–º–µ–Ω–µ–Ω–∏—è:**

```python
# –ë–´–õ–û:
return self.model(
    pixel_values=pixel_values,  # ‚ùå
    input_ids=input_ids,
    attention_mask=attention_mask,
    labels=labels
)

# –î–û–õ–ñ–ù–û –ë–´–¢–¨:
return self.model(
    images=pixel_values,  # ‚úÖ –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞—Ç—å
    input_ids=input_ids,
    attention_mask=attention_mask,
    labels=labels
)
```

#### 3. –í compute_loss (–ü–†–ò–û–†–ò–¢–ï–¢ 2)

**–§–∞–π–ª:** `utils/trainer.py`, –∫–ª–∞—Å—Å `DSModelTrainer`, –º–µ—Ç–æ–¥ `compute_loss()`

**–ò–∑–º–µ–Ω–µ–Ω–∏—è:**

```python
# –ë–´–õ–û:
model_inputs = {}
if "pixel_values" in inputs:
    model_inputs["pixel_values"] = inputs["pixel_values"]

# –î–û–õ–ñ–ù–û –ë–´–¢–¨:
model_inputs = {}
if "images" in inputs:  # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞—Ç—å pixel_values ‚Üí images
    model_inputs["images"] = inputs["images"]
```

### –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π

1. **–ü–†–ò–û–†–ò–¢–ï–¢ 1 (–ö–†–ò–¢–ò–ß–ù–û):**
   - –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞—Ç—å `pixel_values` ‚Üí `images` –≤ `_data_collator()`
   - –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞—Ç—å `pixel_values` ‚Üí `images` –≤ `model_wrapper.py`

2. **–ü–†–ò–û–†–ò–¢–ï–¢ 2 (–í–ê–ñ–ù–û):**
   - –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞—Ç—å –≤ `compute_loss()`
   - –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å, —Ä–∞–±–æ—Ç–∞—é—Ç –ª–∏ `images_seq_mask` –∏ `images_spatial_crop` –∫–∞–∫ `None`

3. **–ü–†–ò–û–†–ò–¢–ï–¢ 3 (–û–ü–¶–ò–û–ù–ê–õ–¨–ù–û):**
   - –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å `images_seq_mask` –∏ `images_spatial_crop`
   - –ï—Å–ª–∏ –Ω—É–∂–Ω—ã - —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∏—Ö —Å–æ–∑–¥–∞–Ω–∏–µ

---

## 7. –í–û–ü–†–û–°–´ –ö –°–ï–ú–Å–ù–£

1. **–§–æ—Ä–º–∞—Ç `images`:**
   - –î–æ–ª–∂–µ–Ω –ª–∏ —ç—Ç–æ –±—ã—Ç—å —Ç–µ–Ω–∑–æ—Ä –ø–æ—Å–ª–µ `torchvision.transforms` (–∫–∞–∫ —Å–µ–π—á–∞—Å `pixel_values`)?
   - –ò–ª–∏ –Ω—É–∂–µ–Ω –¥—Ä—É–≥–æ–π —Ñ–æ—Ä–º–∞—Ç –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏?

2. **`images_seq_mask` –∏ `images_spatial_crop`:**
   - –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã –ª–∏ –æ–Ω–∏ –¥–ª—è training?
   - –ò–ª–∏ –º–æ–∂–Ω–æ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å `None` (–æ–Ω–∏ –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ)?

3. **Processor –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π:**
   - –ï—Å—Ç—å –ª–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π Vision Processor –¥–ª—è DeepSeek-OCR?
   - –ò–ª–∏ –Ω—É–∂–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤—Ä—É—á–Ω—É—é —á–µ—Ä–µ–∑ torchvision?

4. **–ü—Ä–∏–º–µ—Ä—ã –æ–±—É—á–µ–Ω–∏—è:**
   - –ï—Å—Ç—å –ª–∏ —É —Ç–µ–±—è —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ä–∞–±–æ—á–∏–µ –ø—Ä–∏–º–µ—Ä—ã fine-tuning DeepSeek-OCR?
   - –ò–ª–∏ –Ω—É–∂–Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ GitHub —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è?

---

## 8. –í–´–í–û–î–´

### –ß—Ç–æ –º—ã –∑–Ω–∞–µ–º ‚úÖ

1. ‚úÖ –ú–æ–¥–µ–ª—å –ø—Ä–∏–Ω–∏–º–∞–µ—Ç `images` (–Ω–µ `pixel_values`)
2. ‚úÖ –ú–æ–¥–µ–ª—å —Ç–∞–∫–∂–µ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç `images_seq_mask` –∏ `images_spatial_crop` (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ)
3. ‚úÖ Processor - —ç—Ç–æ —Ç–æ–ª—å–∫–æ tokenizer, –Ω–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
4. ‚úÖ –ü–æ–ª–Ω–∞—è —Å–∏–≥–Ω–∞—Ç—É—Ä–∞ `model.forward()` –∏–∑–≤–µ—Å—Ç–Ω–∞

### –ß—Ç–æ –º—ã –ù–ï –∑–Ω–∞–µ–º ‚ùì

1. ‚ùì –§–æ—Ä–º–∞—Ç —Ç–µ–Ω–∑–æ—Ä–∞ `images` (—Ñ–æ—Ä–º–∞—Ç –ø–æ—Å–ª–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏)
2. ‚ùì –ù—É–∂–Ω—ã –ª–∏ `images_seq_mask` –∏ `images_spatial_crop` –¥–ª—è training
3. ‚ùì –ö–∞–∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ —Å–æ–∑–¥–∞—Ç—å —ç—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
4. ‚ùì –ï—Å—Ç—å –ª–∏ –ø—Ä–∏–º–µ—Ä—ã –æ–±—É—á–µ–Ω–∏—è –≤ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏

### –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏

1. **–ù–µ–º–µ–¥–ª–µ–Ω–Ω–æ:**
   - –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞—Ç—å `pixel_values` ‚Üí `images` –≤ –∫–æ–¥–µ
   - –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å, —Ä–∞–±–æ—Ç–∞–µ—Ç –ª–∏ –æ–±—É—á–µ–Ω–∏–µ —Å `images` –∏ –±–µ–∑ `images_seq_mask`/`images_spatial_crop`

2. **–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ:**
   - –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å GitHub —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π DeepSeek-OCR
   - –ò—Å–∫–∞—Ç—å –ø—Ä–∏–º–µ—Ä—ã –æ–±—É—á–µ–Ω–∏—è/fine-tuning
   - –ò–∑—É—á–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é HuggingFace

3. **–ï—Å–ª–∏ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç:**
   - –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å, –∫–∞–∫ —Å–æ–∑–¥–∞–≤–∞—Ç—å `images_seq_mask` –∏ `images_spatial_crop`
   - –ò–∑—É—á–∏—Ç—å Technical Report

---

## –ü–†–ò–õ–û–ñ–ï–ù–ò–Ø

### –õ–æ–≥ test_model_api.py

```
================================================================================
–ò–°–°–õ–ï–î–û–í–ê–ù–ò–ï API DeepSeek-OCR
================================================================================

[OK] –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: <class 'transformers_modules.deepseek-ai.DeepSeek-OCR.9f30c71f441d010e5429c532364a86705536c53a.modeling_deepseekocr.DeepseekOCRForCausalLM'>
[OK] –ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω: <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>

================================================================================
–ò–°–°–õ–ï–î–û–í–ê–ù–ò–ï 2: –°–∏–≥–Ω–∞—Ç—É—Ä–∞ model.forward()
================================================================================

–ü–∞—Ä–∞–º–µ—Ç—Ä—ã model.forward():
  - input_ids: <class 'torch.LongTensor'> = None
  - attention_mask: typing.Optional[torch.Tensor] = None
  - position_ids: typing.Optional[torch.LongTensor] = None
  - past_key_values: typing.Optional[typing.List[torch.FloatTensor]] = None
  - inputs_embeds: typing.Optional[torch.FloatTensor] = None
  - labels: typing.Optional[torch.LongTensor] = None
  - use_cache: typing.Optional[bool] = None
  - output_attentions: typing.Optional[bool] = None
  - output_hidden_states: typing.Optional[bool] = None
  - images: typing.Optional[torch.FloatTensor] = None
  - images_seq_mask: typing.Optional[torch.FloatTensor] = None
  - images_spatial_crop: typing.Optional[torch.FloatTensor] = None
  - return_dict: typing.Optional[bool] = None
```

---

**–° —É–≤–∞–∂–µ–Ω–∏–µ–º,**  
**–ù–∏–∫–æ–ª–∞–π (Cursor AI)** üéØ

P.S. –û—Ç–∫—Ä—ã—Ç–∏–µ –±—ã–ª–æ —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω—ã–º - `pixel_values` vs `images`! –≠—Ç–æ –æ–±—ä—è—Å–Ω—è–µ—Ç –≤—Å–µ –æ—à–∏–±–∫–∏. –¢–µ–ø–µ—Ä—å –Ω—É–∂–Ω–æ –ø—Ä–æ—Å—Ç–æ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞—Ç—å –∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å! üí™
