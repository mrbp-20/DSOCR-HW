"""
–¢–µ—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ data_collator.

–ü—Ä–æ–≤–µ—Ä—è–µ—Ç:
1. –†–∞–∑–¥–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É images –∏ text
2. –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Ñ–æ—Ä–º–∞—Ç–∞ –±–∞—Ç—á–∞
3. –ö–æ–¥–∏—Ä–æ–≤–∫—É –≤—ã–≤–æ–¥–∞
"""

import sys
import torch
from pathlib import Path
from PIL import Image
from transformers import AutoProcessor

# –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É –¥–ª—è Windows
if sys.platform == 'win32':
    if sys.stdout.encoding != 'utf-8':
        sys.stdout.reconfigure(encoding='utf-8')

print("=" * 80)
print("üß™ –¢–ï–°–¢: Data Collator –¥–ª—è DeepSeek-OCR")
print("=" * 80)

# 1. –ó–∞–≥—Ä—É–∑–∫–∞ processor
print("\n1Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∫–∞ processor...")
processor = AutoProcessor.from_pretrained(
    "deepseek-ai/DeepSeek-OCR",
    revision="9f30c71f441d010e5429c532364a86705536c53a",
    trust_remote_code=True
)
print(f"   –¢–∏–ø processor: {type(processor)}")
print(f"   –ï—Å—Ç—å tokenizer: {hasattr(processor, 'tokenizer')}")

# 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
print("\n2Ô∏è‚É£ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
test_images = list(Path("data/raw").glob("*.jpg"))[:2]  # –ë–µ—Ä—ë–º 2 –æ–±—Ä–∞–∑—Ü–∞
test_texts = [
    "–ú–∏–Ω–æ–≤–∞–ª–æ –ø–æ—á—Ç–∏ –¥–µ—Å—è—Ç–∏–ª–µ—Ç–∏–µ...",
    "–û—Ç–ª–∏—á–Ω–∞—è —Ä–∞–±–æ—Ç–∞!"
]

print(f"   –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(test_images)}")
print(f"   –¢–µ–∫—Å—Ç–æ–≤: {len(test_texts)}")

# 3. –¢–µ—Å—Ç —Ä–∞–∑–¥–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
print("\n3Ô∏è‚É£ –¢–µ—Å—Ç —Ä–∞–∑–¥–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏...")

# 3a. Images —á–µ—Ä–µ–∑ processor
print("   3a. –û–±—Ä–∞–±–æ—Ç–∫–∞ images —á–µ—Ä–µ–∑ processor...")
try:
    images = [Image.open(img).convert('RGB') for img in test_images]
    pixel_inputs = processor(images=images, return_tensors="pt")
    print(f"       ‚úÖ pixel_values shape: {pixel_inputs['pixel_values'].shape}")
except Exception as e:
    print(f"       ‚ùå –û—à–∏–±–∫–∞: {e}")

# 3b. Text —á–µ—Ä–µ–∑ processor.batch_encode_plus
print("   3b. –û–±—Ä–∞–±–æ—Ç–∫–∞ text —á–µ—Ä–µ–∑ processor.batch_encode_plus...")
try:
    text_inputs = processor.batch_encode_plus(
        test_texts,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=512
    )
    print(f"       ‚úÖ input_ids shape: {text_inputs['input_ids'].shape}")
    print(f"       ‚úÖ attention_mask shape: {text_inputs['attention_mask'].shape}")
except Exception as e:
    print(f"       ‚ùå –û—à–∏–±–∫–∞: {e}")

# 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏
print("\n5Ô∏è‚É£ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏...")
print("   –†—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç: –ü—Ä–∏–≤–µ—Ç, –º–∏—Ä! üöÄ")
print("   Emoji: üí™ ‚úÖ ‚ùå üéØ")

print("\n" + "=" * 80)
print("‚úÖ –¢–ï–°–¢ –ó–ê–í–ï–†–®–Å–ù!")
print("=" * 80)
