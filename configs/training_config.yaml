# Training configuration for DSOCR-HW
# Конфигурация fine-tuning DeepSeek-OCR с LoRA

model:
  base_model: "deepseek-ai/DeepSeek-OCR"
  cache_dir: "models/base"  # Кэш HuggingFace моделей
  torch_dtype: "float16"     # fp16 для экономии VRAM
  attn_implementation: "eager"  # ВАЖНО: для sm_120 (RTX 5060 Ti)
  device_map: "auto"         # Автоматическое распределение по GPU
  trust_remote_code: true    # Для DeepSeek-OCR

lora:
  r: 8                       # Rank LoRA (4-16, чем больше → точнее, но тяжелее)
  lora_alpha: 16             # Scaling factor (обычно 2*r)
  lora_dropout: 0.05         # Dropout для регуляризации
  bias: "none"               # Не обучаем bias
  task_type: "SEQ_2_SEQ_LM" # Тип задачи
  target_modules:            # Модули для LoRA (обычно attention + MLP)
    - "q_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"

training:
  output_dir: "models/lora_adapters/handwriting_v1"
  num_train_epochs: 5  # Proof-of-concept: проверяем, что всё работает
  per_device_train_batch_size: 1  # У нас всего 3 train образца
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 2  # Эффективный batch_size = 1*2 = 2
  learning_rate: 3.0e-4
  weight_decay: 0.01
  warmup_steps: 100
  logging_steps: 10
  save_steps: 3  # Сохраняем каждую эпоху (3 шага = 1 эпоха)
  save_total_limit: 3        # Храним только 3 последних чекпоинта
  eval_steps: 3  # Оценка каждую эпоху
  evaluation_strategy: "steps"
  save_strategy: "steps"
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false
  fp16: false                # Отключено: конфликтует с LoRA (используем FP32)
  dataloader_num_workers: 0  # Отключено для Windows + custom code моделей (multiprocessing проблемы)
  remove_unused_columns: false
  report_to: "tensorboard"   # Логирование в TensorBoard
  logging_dir: "models/lora_adapters/handwriting_v1/runs"
  seed: 42

optimization:
  optimizer: "adamw_torch"   # Оптимизатор
  lr_scheduler_type: "cosine"  # Cosine learning rate schedule
  max_grad_norm: 1.0         # Gradient clipping

data:
  train_path: "data/processed/train"
  val_path: "data/processed/val"
  max_seq_length: 512        # Максимальная длина токенов
  preprocessing:
    resize: [224, 224]       # Размер изображений (если нужно)
    normalize: true
  augmentation:
    enabled: true
    rotation_range: 5        # Случайный поворот ±5 градусов
    brightness_range: [0.8, 1.2]
    contrast_range: [0.8, 1.2]

early_stopping:
  patience: 5                # Остановка, если eval_loss не улучшается 5 эпох
  threshold: 0.01
