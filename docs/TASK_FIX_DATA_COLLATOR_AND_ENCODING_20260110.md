# üîß –ó–ê–î–ê–ù–ò–ï: –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï DATA_COLLATOR –ò –ö–û–î–ò–†–û–í–ö–ò –ö–û–ù–°–û–õ–ò

**–ê–≤—Ç–æ—Ä:** –°–µ–º—ë–Ω (Tech Lead)  
**–î–∞—Ç–∞:** 2026-01-10, 16:27 MSK  
**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** üî• CRITICAL (–±–ª–æ–∫–∏—Ä—É–µ—Ç –∑–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è)  
**–°—Ä–æ–∫:** 30-40 –º–∏–Ω—É—Ç  
**–ò—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å:** –ù–∏–∫–æ–ª–∞–π (Senior ML Engineer)  
**–°–≤—è–∑–∞–Ω–æ —Å:** REPORT_TRAINING_PROGRESS_BARS_20260110.md, Issue #2

---

## üéØ –¶–ï–õ–¨

–ò—Å–ø—Ä–∞–≤–∏—Ç—å –¥–≤–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã, –±–ª–æ–∫–∏—Ä—É—é—â–∏–µ –æ–±—É—á–µ–Ω–∏–µ:

1. **DATA_COLLATOR API** ‚Äî –æ—à–∏–±–∫–∞ `TypeError: PreTrainedTokenizerFast._batch_encode_plus() got an unexpected keyword argument 'images'`
2. **–ö–û–î–ò–†–û–í–ö–ê –ö–û–ù–°–û–õ–ò** ‚Äî –ø—Ä–æ–±–ª–µ–º—ã —Å –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –∏ emoji –≤ Windows Terminal

---

## üìã –ü–†–û–ë–õ–ï–ú–ê #1: DATA_COLLATOR API

### –î–∏–∞–≥–Ω–æ–∑

–¢–µ–∫—É—â–∏–π –∫–æ–¥ –≤ `utils/trainer.py` (–º–µ—Ç–æ–¥ `_data_collator()`) –ø—ã—Ç–∞–µ—Ç—Å—è –ø–µ—Ä–µ–¥–∞—Ç—å **images –∏ text –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ** –≤ `processor()`:

```python
# ‚ùå –ù–ï –†–ê–ë–û–¢–ê–ï–¢
inputs = self.processor(images=images, text=texts, return_tensors="pt", padding=True)
```

**–ü—Ä–∏—á–∏–Ω–∞:**  
`AutoProcessor.from_pretrained("deepseek-ai/DeepSeek-OCR")` –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç `LlamaTokenizerFast`, –∫–æ—Ç–æ—Ä—ã–π **–Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä `images`**.

### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ DeepSeek-OCR

DeepSeek-OCR —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –¥–≤—É—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤:
- **DeepEncoder (Vision)** ‚Äî –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è ‚Üí vision tokens
- **DeepSeek-3B-MoE (Text)** ‚Äî –¥–µ–∫–æ–¥–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç

–î–ª—è –æ–±—É—á–µ–Ω–∏—è Vision2Seq –º–æ–¥–µ–ª–µ–π –Ω—É–∂–Ω–∞ **—Ä–∞–∑–¥–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞**:
- **Images** ‚Üí —á–µ—Ä–µ–∑ `processor(images=...)`
- **Text** ‚Üí —á–µ—Ä–µ–∑ `processor.tokenizer(text=...)`

–≠—Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –≤—Å–µ—Ö VLM (Vision Language Models).

---

## üõ†Ô∏è –†–ï–®–ï–ù–ò–ï #1: –†–ê–ó–î–ï–õ–¨–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê IMAGES –ò TEXT

### –í–∞—Ä–∏–∞–Ω—Ç A: –†—É—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤ data_collator (–†–ï–ö–û–ú–ï–ù–î–£–Æ)

**–§–∞–π–ª:** `utils/trainer.py`  
**–ú–µ—Ç–æ–¥:** `_data_collator()`

**–ó–∞–º–µ–Ω–∏—Ç—å —Ç–µ–∫—É—â–∏–π –∫–æ–¥ –Ω–∞:**

```python
def _data_collator(self, examples: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Data collator –¥–ª—è DeepSeek-OCR —Å —Ä–∞–∑–¥–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π images –∏ text.
    
    DeepSeek-OCR —Ç—Ä–µ–±—É–µ—Ç:
    1. Processor –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (pixel_values)
    2. Tokenizer –¥–ª—è —Ç–µ–∫—Å—Ç–∞ (input_ids, attention_mask, labels)
    
    Args:
        examples: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∫–ª—é—á–∞–º–∏ 'image_path' –∏ 'text'
    
    Returns:
        –ë–∞—Ç—á –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –∫–ª—é—á–∞–º–∏:
        - pixel_values: —Ç–µ–Ω–∑–æ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        - input_ids: —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        - attention_mask: –º–∞—Å–∫–∞ –≤–Ω–∏–º–∞–Ω–∏—è
        - labels: –º–µ—Ç–∫–∏ –¥–ª—è loss (–∫–æ–ø–∏—è input_ids)
    """
    from PIL import Image
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    images = [Image.open(ex['image_path']).convert('RGB') for ex in examples]
    texts = [ex['text'] for ex in examples]
    
    # 2. –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ processor (—Ç–æ–ª—å–∫–æ images!)
    try:
        pixel_inputs = self.processor(images=images, return_tensors="pt")
    except Exception as e:
        self.logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {e}")
        raise
    
    # 3. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ processor.tokenizer
    try:
        text_inputs = self.processor.tokenizer(
            texts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=self.training_config.get('max_seq_length', 512)
        )
    except Exception as e:
        self.logger.error(f"–û—à–∏–±–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞: {e}")
        raise
    
    # 4. –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö inputs –≤ –æ–¥–∏–Ω –±–∞—Ç—á
    batch = {
        **pixel_inputs,  # pixel_values, etc.
        'input_ids': text_inputs['input_ids'],
        'attention_mask': text_inputs['attention_mask'],
    }
    
    # 5. Labels = input_ids –¥–ª—è teacher forcing (—Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è seq2seq)
    # –ö–æ–ø–∏—Ä—É–µ–º, —á—Ç–æ–±—ã –Ω–µ –∏–∑–º–µ–Ω—è—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–Ω–∑–æ—Ä
    batch['labels'] = text_inputs['input_ids'].clone()
    
    return batch
```

### –í–∞—Ä–∏–∞–Ω—Ç B: –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –≤ prepare_datasets() (–ê–õ–¨–¢–ï–†–ù–ê–¢–ò–í–ê)

–ï—Å–ª–∏ —Ö–æ—á–µ—à—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π `DataCollatorForSeq2Seq` –∏–∑ transformers, —Ç–æ –Ω—É–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å –º–µ—Ç–æ–¥ `prepare_datasets()`:

**–§–∞–π–ª:** `utils/trainer.py`  
**–ú–µ—Ç–æ–¥:** `prepare_datasets()`

```python
def prepare_datasets(self, train_path: str, val_path: str):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏—Ä—É–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç—ã —Å —Ä–∞–∑–¥–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π images –∏ text.
    """
    from datasets import Dataset
    from PIL import Image
    
    def preprocess_function(examples):
        """
        –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –æ–¥–Ω–æ–≥–æ –±–∞—Ç—á–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞.
        
        –†–∞–∑–¥–µ–ª—è–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∫—É:
        - images ‚Üí processor
        - text ‚Üí tokenizer
        """
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        images = [Image.open(path).convert('RGB') for path in examples['image_path']]
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ processor (—Ç–æ–ª—å–∫–æ images)
        pixel_inputs = self.processor(images=images, return_tensors="pt")
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
        text_inputs = self.processor.tokenizer(
            examples['text'],
            padding='max_length',
            truncation=True,
            max_length=self.training_config.get('max_seq_length', 512),
            return_tensors="pt"
        )
        
        return {
            **pixel_inputs,
            'input_ids': text_inputs['input_ids'],
            'attention_mask': text_inputs['attention_mask'],
            'labels': text_inputs['input_ids']  # –¥–ª—è loss
        }
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑ metadata.json
    train_dataset = Dataset.from_json(train_path)
    val_dataset = Dataset.from_json(val_path)
    
    # –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥
    self.logger.info("–ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ train –¥–∞—Ç–∞—Å–µ—Ç–∞...")
    train_dataset = train_dataset.map(
        preprocess_function, 
        batched=True,
        remove_columns=['image_path', 'text']  # —É–¥–∞–ª—è–µ–º, –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ–Ω–∑–æ—Ä—ã
    )
    
    self.logger.info("–ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ val –¥–∞—Ç–∞—Å–µ—Ç–∞...")
    val_dataset = val_dataset.map(
        preprocess_function, 
        batched=True,
        remove_columns=['image_path', 'text']
    )
    
    return train_dataset, val_dataset
```

**–¢–æ–≥–¥–∞ –≤ `create_trainer()` –∏—Å–ø–æ–ª—å–∑—É–π –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π collator:**

```python
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(
    tokenizer=self.processor.tokenizer,
    model=self.model,
    padding=True,
    pad_to_multiple_of=8  # –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –Ω–∞ GPU
)
```

### –ö–∞–∫–æ–π –≤–∞—Ä–∏–∞–Ω—Ç –≤—ã–±—Ä–∞—Ç—å?

| –í–∞—Ä–∏–∞–Ω—Ç | –ü–ª—é—Å—ã | –ú–∏–Ω—É—Å—ã | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è |
|---------|-------|--------|--------------|
| **A: –†—É—á–Ω–æ–π collator** | –ü—Ä–æ—â–µ, –º–µ–Ω—å—à–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π, –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å | –ù—É–∂–Ω–æ –≤—Ä—É—á–Ω—É—é —É–ø—Ä–∞–≤–ª—è—Ç—å padding | ‚úÖ **–î–ê** (–¥–ª—è MVP) |
| **B: –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ + –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π collator** | –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–π HF –∫–æ–¥ | –ë–æ–ª—å—à–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π, —Å–ª–æ–∂–Ω–µ–µ –æ—Ç–ª–∞–¥–∫–∞ | ‚ö†Ô∏è –î–ª—è production |

**–î–ª—è —Ç–µ–∫—É—â–µ–π –∑–∞–¥–∞—á–∏ –≤—ã–±–µ—Ä–∏ –í–∞—Ä–∏–∞–Ω—Ç A** ‚Äî –æ–Ω –±—ã—Å—Ç—Ä–µ–µ –∏ –ø—Ä–æ—â–µ.

---

## üõ†Ô∏è –†–ï–®–ï–ù–ò–ï #2: –ö–û–î–ò–†–û–í–ö–ê –ö–û–ù–°–û–õ–ò (WINDOWS)

### –ü—Ä–æ–±–ª–µ–º–∞

Windows PowerShell –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–æ–¥–∏—Ä–æ–≤–∫—É **Windows-1251** (–∏–ª–∏ CP866 –≤ cmd), —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫:
- –ö—Ä—è–∫–æ–∑—è–±–ª–∞–º –≤–º–µ—Å—Ç–æ —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
- –û—à–∏–±–∫–∞–º –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è emoji (üí™, üöÄ, ‚úÖ)
- –ü—Ä–æ–±–ª–µ–º–∞–º —Å progress bars –æ—Ç tqdm

### –†–µ—à–µ–Ω–∏–µ: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞ UTF-8

**–§–∞–π–ª:** `scripts/train_lora.py`  
**–ú–µ—Å—Ç–æ:** –í —Å–∞–º–æ–º –Ω–∞—á–∞–ª–µ `main()`, –ø–µ—Ä–µ–¥ –ª—é–±—ã–º –≤—ã–≤–æ–¥–æ–º

**–î–æ–±–∞–≤–∏—Ç—å:**

```python
def main():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ –æ–±—É—á–µ–Ω–∏—è LoRA.
    """
    # ========================================
    # –ö–†–ò–¢–ò–ß–ù–û: –£—Å—Ç–∞–Ω–æ–≤–∫–∞ UTF-8 –¥–ª—è Windows
    # ========================================
    import sys
    import os
    
    # –î–ª—è Windows: –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º UTF-8
    if sys.platform == 'win32':
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º UTF-8 –¥–ª—è stdout –∏ stderr
        if sys.stdout.encoding != 'utf-8':
            sys.stdout.reconfigure(encoding='utf-8')
        if sys.stderr.encoding != 'utf-8':
            sys.stderr.reconfigure(encoding='utf-8')
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è –ø–æ–¥–ø—Ä–æ—Ü–µ—Å—Å–æ–≤
        os.environ['PYTHONIOENCODING'] = 'utf-8'
        
        # –î–ª—è Windows Terminal: —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Å–æ–ª—å–Ω—ã–π codepage
        try:
            import ctypes
            ctypes.windll.kernel32.SetConsoleCP(65001)
            ctypes.windll.kernel32.SetConsoleOutputCP(65001)
        except Exception:
            pass  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏, –µ—Å–ª–∏ –Ω–µ Windows
    
    # –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –±–µ–∑–æ–ø–∞—Å–Ω–æ –≤—ã–≤–æ–¥–∏—Ç—å —Ä—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç –∏ emoji
    print("=" * 80)
    print("üöÄ DSOCR-HW: –û–±—É—á–µ–Ω–∏–µ DeepSeek-OCR —Å LoRA")
    print("=" * 80)
    
    # ... –æ—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ main() ...
```

### –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç: —Ñ—É–Ω–∫—Ü–∏—è-—É—Ç–∏–ª–∏—Ç–∞

**–§–∞–π–ª:** `utils/encoding_fix.py` (–°–û–ó–î–ê–¢–¨ –ù–û–í–´–ô)

```python
"""
–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–æ–¥–∏—Ä–æ–≤–∫–∏ –∫–æ–Ω—Å–æ–ª–∏ –≤ Windows.

–ü—Ä–æ–±–ª–µ–º–∞: Windows –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ—Ç CP1251/CP866,
—á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –∫—Ä—è–∫–æ–∑—è–±–ª–∞–º –ø—Ä–∏ –≤—ã–≤–æ–¥–µ —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –∏ emoji.

–†–µ—à–µ–Ω–∏–µ: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞ UTF-8 –¥–ª—è –≤—Å–µ—Ö –ø–æ—Ç–æ–∫–æ–≤ –≤—ã–≤–æ–¥–∞.
"""

import sys
import os


def fix_windows_console_encoding():
    """
    –ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç –∫–æ–¥–∏—Ä–æ–≤–∫—É –∫–æ–Ω—Å–æ–ª–∏ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è UTF-8 –≤ Windows.
    
    –í—ã–ø–æ–ª–Ω—è–µ—Ç:
    1. –ü–µ—Ä–µ–∫–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä—É–µ—Ç stdout/stderr –Ω–∞ UTF-8
    2. –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç PYTHONIOENCODING=utf-8
    3. –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç Windows Console Code Page –Ω–∞ 65001 (UTF-8)
    
    –ë–µ–∑–æ–ø–∞—Å–Ω–æ –¥–ª—è –¥—Ä—É–≥–∏—Ö –û–° ‚Äî –Ω–∏—á–µ–≥–æ –Ω–µ –¥–µ–ª–∞–µ—Ç –Ω–∞ Linux/macOS.
    
    Usage:
        from utils.encoding_fix import fix_windows_console_encoding
        
        # –í –Ω–∞—á–∞–ª–µ main():
        fix_windows_console_encoding()
        print("–¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç –∏ emoji! üöÄ")
    """
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ Windows
    if sys.platform != 'win32':
        return  # –ù–∞ Linux/macOS –Ω–∏—á–µ–≥–æ –Ω–µ –¥–µ–ª–∞–µ–º
    
    # 1. –ü–µ—Ä–µ–∫–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä—É–µ–º stdout –∏ stderr
    if sys.stdout.encoding != 'utf-8':
        try:
            sys.stdout.reconfigure(encoding='utf-8')
        except AttributeError:
            # –î–ª—è —Å—Ç–∞—Ä—ã—Ö –≤–µ—Ä—Å–∏–π Python (< 3.7)
            import io
            sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    
    if sys.stderr.encoding != 'utf-8':
        try:
            sys.stderr.reconfigure(encoding='utf-8')
        except AttributeError:
            import io
            sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')
    
    # 2. –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è –ø–æ–¥–ø—Ä–æ—Ü–µ—Å—Å–æ–≤
    os.environ['PYTHONIOENCODING'] = 'utf-8'
    
    # 3. –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º Windows Console Code Page –Ω–∞ UTF-8 (65001)
    try:
        import ctypes
        # SetConsoleCP - –¥–ª—è –≤–≤–æ–¥–∞
        ctypes.windll.kernel32.SetConsoleCP(65001)
        # SetConsoleOutputCP - –¥–ª—è –≤—ã–≤–æ–¥–∞
        ctypes.windll.kernel32.SetConsoleOutputCP(65001)
    except Exception as e:
        # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ (–º–æ–∂–µ—Ç –Ω–µ —Ä–∞–±–æ—Ç–∞—Ç—å –≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Ç–µ—Ä–º–∏–Ω–∞–ª–∞—Ö)
        pass


def print_encoding_info():
    """
    –í—ã–≤–æ–¥–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–µ–∫—É—â–∏—Ö –∫–æ–¥–∏—Ä–æ–≤–∫–∞—Ö –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏.
    
    Usage:
        from utils.encoding_fix import print_encoding_info
        print_encoding_info()
    """
    print("=" * 60)
    print("üìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–æ–¥–∏—Ä–æ–≤–∫–∞—Ö:")
    print("=" * 60)
    print(f"sys.stdout.encoding: {sys.stdout.encoding}")
    print(f"sys.stderr.encoding: {sys.stderr.encoding}")
    print(f"sys.getdefaultencoding(): {sys.getdefaultencoding()}")
    print(f"PYTHONIOENCODING: {os.environ.get('PYTHONIOENCODING', '–Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ')}")
    print(f"–ü–ª–∞—Ç—Ñ–æ—Ä–º–∞: {sys.platform}")
    
    # –¢–µ—Å—Ç –≤—ã–≤–æ–¥–∞
    print("\nüß™ –¢–µ—Å—Ç –≤—ã–≤–æ–¥–∞:")
    print("  - –†—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç: –ü—Ä–∏–≤–µ—Ç, –º–∏—Ä!")
    print("  - Emoji: üöÄ üí™ ‚úÖ ‚ùå üéØ")
    print("  - Progress bar simulation: " + "‚ñà" * 20)
    print("=" * 60)
```

**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ `scripts/train_lora.py`:**

```python
from utils.encoding_fix import fix_windows_console_encoding, print_encoding_info

def main():
    # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É –≤ —Å–∞–º–æ–º –Ω–∞—á–∞–ª–µ
    fix_windows_console_encoding()
    
    # –î–ª—è –æ—Ç–ª–∞–¥–∫–∏ (–º–æ–∂–Ω–æ —É–±—Ä–∞—Ç—å –ø–æ—Ç–æ–º):
    # print_encoding_info()
    
    print("üöÄ DSOCR-HW: –û–±—É—á–µ–Ω–∏–µ DeepSeek-OCR —Å LoRA")
    # ... –æ—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ ...
```

---

## ‚úÖ –ß–ï–ö–õ–ò–°–¢ –í–´–ü–û–õ–ù–ï–ù–ò–Ø

### –ó–∞–¥–∞—á–∞ #1: Data Collator

- [ ] **–ü—Ä–æ—á–∏—Ç–∞–ª –∏ –ø–æ–Ω—è–ª** –ø—Ä–æ–±–ª–µ–º—É —Å processor API
- [ ] **–í—ã–±—Ä–∞–ª –≤–∞—Ä–∏–∞–Ω—Ç** (A ‚Äî —Ä—É—á–Ω–æ–π collator –ò–õ–ò B ‚Äî –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥)
- [ ] **–ò–∑–º–µ–Ω–∏–ª –∫–æ–¥** –≤ `utils/trainer.py`
- [ ] **–î–æ–±–∞–≤–∏–ª –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–æ–∫** (try-except –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ images –∏ text)
- [ ] **–ü—Ä–æ–≤–µ—Ä–∏–ª —Å–∏–Ω—Ç–∞–∫—Å–∏—Å:** `python -m py_compile utils/trainer.py`
- [ ] **–°–¥–µ–ª–∞–ª –∫–æ–º–º–∏—Ç:** `git commit -m "fix: data_collator with separate images/text processing"`

### –ó–∞–¥–∞—á–∞ #2: –ö–æ–¥–∏—Ä–æ–≤–∫–∞ –∫–æ–Ω—Å–æ–ª–∏

- [ ] **–°–æ–∑–¥–∞–ª —Ñ–∞–π–ª** `utils/encoding_fix.py` —Å —Ñ—É–Ω–∫—Ü–∏—è–º–∏ (–µ—Å–ª–∏ –≤—ã–±—Ä–∞–ª —É—Ç–∏–ª–∏—Ç—É)
- [ ] **–î–æ–±–∞–≤–∏–ª fix –≤ –Ω–∞—á–∞–ª–æ** `scripts/train_lora.py` (–≤ —Ñ—É–Ω–∫—Ü–∏—é `main()`)
- [ ] **–ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–ª –≤—ã–≤–æ–¥** —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –∏ emoji
- [ ] **–ü—Ä–æ–≤–µ—Ä–∏–ª —Å–∏–Ω—Ç–∞–∫—Å–∏—Å:** `python -m py_compile utils/encoding_fix.py`
- [ ] **–°–¥–µ–ª–∞–ª –∫–æ–º–º–∏—Ç:** `git commit -m "fix: Windows console encoding for UTF-8 support"`

### –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

- [ ] **–ó–∞–ø—É—Å—Ç–∏–ª —Å–∫—Ä–∏–ø—Ç** —Å —Ñ–∏–∫—Å–∞–º–∏:
  ```powershell
  python scripts/train_lora.py --config configs/training_config.yaml
  ```
- [ ] **–ü—Ä–æ–≤–µ—Ä–∏–ª, —á—Ç–æ –Ω–µ—Ç –æ—à–∏–±–∫–∏** `TypeError: ... got an unexpected keyword argument 'images'`
- [ ] **–ü—Ä–æ–≤–µ—Ä–∏–ª, —á—Ç–æ —Ä—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç** –æ—Ç–æ–±—Ä–∞–∂–∞–µ—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ (–Ω–µ –∫—Ä—è–∫–æ–∑—è–±—Ä—ã)
- [ ] **–ü—Ä–æ–≤–µ—Ä–∏–ª, —á—Ç–æ emoji** –æ—Ç–æ–±—Ä–∞–∂–∞—é—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ (üöÄ üí™ ‚úÖ)
- [ ] **–ü—Ä–æ–≤–µ—Ä–∏–ª, —á—Ç–æ progress bars** –æ—Ç tqdm –æ—Ç–æ–±—Ä–∞–∂–∞—é—Ç—Å—è –Ω–æ—Ä–º–∞–ª—å–Ω–æ

### –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è

- [ ] **–°–æ–∑–¥–∞–ª –æ—Ç—á—ë—Ç** `REPORT_FIX_DATA_COLLATOR_20260110.md` —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
- [ ] **–û–±–Ω–æ–≤–∏–ª Issue #2** —Å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–º –æ —Å—Ç–∞—Ç—É—Å–µ
- [ ] **–°–¥–µ–ª–∞–ª —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –∫–æ–º–º–∏—Ç** —Å –æ—Ç—á—ë—Ç–æ–º
- [ ] **–ó–∞–ø—É—Å—Ç–∏–ª –æ–±—É—á–µ–Ω–∏–µ** –Ω–∞ 1 —ç–ø–æ—Ö–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏

---

## üß™ –¢–ï–°–¢–û–í–´–ô –°–¶–ï–ù–ê–†–ò–ô

### –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç (–ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º –æ–±—É—á–µ–Ω–∏—è)

**–§–∞–π–ª:** `test_data_collator.py` (—Å–æ–∑–¥–∞—Ç—å –≤ –∫–æ—Ä–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞)

```python
"""
–¢–µ—Å—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ data_collator.

–ü—Ä–æ–≤–µ—Ä—è–µ—Ç:
1. –†–∞–∑–¥–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É images –∏ text
2. –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Ñ–æ—Ä–º–∞—Ç–∞ –±–∞—Ç—á–∞
3. –ö–æ–¥–∏—Ä–æ–≤–∫—É –≤—ã–≤–æ–¥–∞
"""

import sys
import torch
from pathlib import Path
from PIL import Image
from transformers import AutoProcessor

# –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É –¥–ª—è Windows
if sys.platform == 'win32':
    if sys.stdout.encoding != 'utf-8':
        sys.stdout.reconfigure(encoding='utf-8')

print("=" * 80)
print("üß™ –¢–ï–°–¢: Data Collator –¥–ª—è DeepSeek-OCR")
print("=" * 80)

# 1. –ó–∞–≥—Ä—É–∑–∫–∞ processor
print("\n1Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∫–∞ processor...")
processor = AutoProcessor.from_pretrained(
    "deepseek-ai/DeepSeek-OCR",
    trust_remote_code=True
)
print(f"   –¢–∏–ø processor: {type(processor)}")
print(f"   –ï—Å—Ç—å tokenizer: {hasattr(processor, 'tokenizer')}")

# 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
print("\n2Ô∏è‚É£ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
test_images = list(Path("data/raw").glob("*.jpg"))[:2]  # –ë–µ—Ä—ë–º 2 –æ–±—Ä–∞–∑—Ü–∞
test_texts = [
    "–ú–∏–Ω–æ–≤–∞–ª–æ –ø–æ—á—Ç–∏ –¥–µ—Å—è—Ç–∏–ª–µ—Ç–∏–µ...",
    "–û—Ç–ª–∏—á–Ω–∞—è —Ä–∞–±–æ—Ç–∞!"
]

print(f"   –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(test_images)}")
print(f"   –¢–µ–∫—Å—Ç–æ–≤: {len(test_texts)}")

# 3. –¢–µ—Å—Ç —Ä–∞–∑–¥–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
print("\n3Ô∏è‚É£ –¢–µ—Å—Ç —Ä–∞–∑–¥–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏...")

# 3a. Images —á–µ—Ä–µ–∑ processor
print("   3a. –û–±—Ä–∞–±–æ—Ç–∫–∞ images —á–µ—Ä–µ–∑ processor...")
images = [Image.open(img).convert('RGB') for img in test_images]
pixel_inputs = processor(images=images, return_tensors="pt")
print(f"       ‚úÖ pixel_values shape: {pixel_inputs['pixel_values'].shape}")

# 3b. Text —á–µ—Ä–µ–∑ processor.tokenizer
print("   3b. –û–±—Ä–∞–±–æ—Ç–∫–∞ text —á–µ—Ä–µ–∑ processor.tokenizer...")
text_inputs = processor.tokenizer(
    test_texts,
    return_tensors="pt",
    padding=True,
    truncation=True,
    max_length=512
)
print(f"       ‚úÖ input_ids shape: {text_inputs['input_ids'].shape}")
print(f"       ‚úÖ attention_mask shape: {text_inputs['attention_mask'].shape}")

# 4. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –±–∞—Ç—á–∞
print("\n4Ô∏è‚É£ –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –±–∞—Ç—á–∞...")
batch = {
    **pixel_inputs,
    'input_ids': text_inputs['input_ids'],
    'attention_mask': text_inputs['attention_mask'],
    'labels': text_inputs['input_ids'].clone()
}

print("   –ö–ª—é—á–∏ –±–∞—Ç—á–∞:")
for key, value in batch.items():
    if isinstance(value, torch.Tensor):
        print(f"     - {key}: shape {value.shape}, dtype {value.dtype}")
    else:
        print(f"     - {key}: {type(value)}")

# 5. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏
print("\n5Ô∏è‚É£ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏...")
print("   –†—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç: –ü—Ä–∏–≤–µ—Ç, –º–∏—Ä! üöÄ")
print("   Emoji: üí™ ‚úÖ ‚ùå üéØ")

print("\n" + "=" * 80)
print("‚úÖ –í–°–ï –¢–ï–°–¢–´ –ü–†–û–ô–î–ï–ù–´!")
print("=" * 80)
```

**–ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–∞:**

```powershell
cd C:\DSOCR-HW
.\venv\Scripts\Activate.ps1
python test_data_collator.py
```

**–û–∂–∏–¥–∞–µ–º—ã–π –≤—ã–≤–æ–¥:**

```
================================================================================
üß™ –¢–ï–°–¢: Data Collator –¥–ª—è DeepSeek-OCR
================================================================================

1Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∫–∞ processor...
   –¢–∏–ø processor: <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>
   –ï—Å—Ç—å tokenizer: False

2Ô∏è‚É£ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...
   –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: 2
   –¢–µ–∫—Å—Ç–æ–≤: 2

3Ô∏è‚É£ –¢–µ—Å—Ç —Ä–∞–∑–¥–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏...
   3a. –û–±—Ä–∞–±–æ—Ç–∫–∞ images —á–µ—Ä–µ–∑ processor...
       ‚úÖ pixel_values shape: torch.Size([2, 3, 1024, 1024])
   3b. –û–±—Ä–∞–±–æ—Ç–∫–∞ text —á–µ—Ä–µ–∑ processor.tokenizer...
       ‚úÖ input_ids shape: torch.Size([2, 12])
       ‚úÖ attention_mask shape: torch.Size([2, 12])

4Ô∏è‚É£ –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –±–∞—Ç—á–∞...
   –ö–ª—é—á–∏ –±–∞—Ç—á–∞:
     - pixel_values: shape torch.Size([2, 3, 1024, 1024]), dtype torch.float32
     - input_ids: shape torch.Size([2, 12]), dtype torch.int64
     - attention_mask: shape torch.Size([2, 12]), dtype torch.int64
     - labels: shape torch.Size([2, 12]), dtype torch.int64

5Ô∏è‚É£ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏...
   –†—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç: –ü—Ä–∏–≤–µ—Ç, –º–∏—Ä! üöÄ
   Emoji: üí™ ‚úÖ ‚ùå üéØ

================================================================================
‚úÖ –í–°–ï –¢–ï–°–¢–´ –ü–†–û–ô–î–ï–ù–´!
================================================================================
```

---

## üìä –û–ñ–ò–î–ê–ï–ú–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´

–ü–æ—Å–ª–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞–Ω–∏—è:

1. **–û–±—É—á–µ–Ω–∏–µ –∑–∞–ø—É—Å—Ç–∏—Ç—Å—è –±–µ–∑ –æ—à–∏–±–æ–∫:**
   ```
   Epoch 1/5:   0%|                                    | 0/3 [00:00<?, ?it/s]
   ```

2. **–ö–æ–Ω—Å–æ–ª—å –±—É–¥–µ—Ç –æ—Ç–æ–±—Ä–∞–∂–∞—Ç—å —Ä—É—Å—Å–∫–∏–π —Ç–µ–∫—Å—Ç –∏ emoji:**
   ```
   ‚úÖ –®–∞–≥ 1 –∑–∞–≤–µ—Ä—à—ë–Ω: –ú–æ–¥–µ–ª—å –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω—ã
   üöÄ –®–∞–≥ 2: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA...
   üí™ –®–∞–≥ 3: –°–æ–∑–¥–∞–Ω–∏–µ Trainer...
   ```

3. **Progress bars –æ—Ç tqdm –±—É–¥—É—Ç —Ä–∞–±–æ—Ç–∞—Ç—å:**
   ```
   –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00]
   ```

4. **–í –ª–æ–≥–∞—Ö –±—É–¥–µ—Ç –≤–∏–¥–Ω–æ –ø—Ä–æ–≥—Ä–µ—Å—Å:**
   ```
   2026-01-10 16:30:00 - train_lora - INFO - Epoch 1, Step 1/3, Loss: 2.456
   ```

---

## üö® –í–û–ó–ú–û–ñ–ù–´–ï –ü–†–û–ë–õ–ï–ú–´ –ò –†–ï–®–ï–ù–ò–Ø

### –ü—Ä–æ–±–ª–µ–º–∞ 1: "AttributeError: 'LlamaTokenizerFast' object has no attribute 'tokenizer'"

**–ü—Ä–∏—á–∏–Ω–∞:** –£ `processor` –Ω–µ—Ç –∞—Ç—Ä–∏–±—É—Ç–∞ `.tokenizer`, –ø–æ—Ç–æ–º—É —á—Ç–æ –æ–Ω –°–ê–ú —è–≤–ª—è–µ—Ç—Å—è tokenizer.

**–†–µ—à–µ–Ω–∏–µ:** –í–º–µ—Å—Ç–æ `processor.tokenizer` –∏—Å–ø–æ–ª—å–∑—É–π –ø—Ä–æ—Å—Ç–æ `processor`:

```python
# ‚ùå –ù–ï –†–ê–ë–û–¢–ê–ï–¢
text_inputs = self.processor.tokenizer(texts, ...)

# ‚úÖ –†–ê–ë–û–¢–ê–ï–¢
text_inputs = self.processor(texts, ...)  # processor –°–ê–ú —è–≤–ª—è–µ—Ç—Å—è tokenizer
```

### –ü—Ä–æ–±–ª–µ–º–∞ 2: "RuntimeError: Expected all tensors to be on the same device"

**–ü—Ä–∏—á–∏–Ω–∞:** `pixel_values` –∏ `input_ids` –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö (CPU vs GPU).

**–†–µ—à–µ–Ω–∏–µ:** –î–æ–±–∞–≤–∏—Ç—å `.to(device)` –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏:

```python
device = self.model.device

batch = {
    'pixel_values': pixel_inputs['pixel_values'].to(device),
    'input_ids': text_inputs['input_ids'].to(device),
    'attention_mask': text_inputs['attention_mask'].to(device),
    'labels': text_inputs['input_ids'].clone().to(device)
}
```

### –ü—Ä–æ–±–ª–µ–º–∞ 3: "–ö–æ–Ω—Å–æ–ª—å –≤—Å—ë —Ä–∞–≤–Ω–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∫—Ä—è–∫–æ–∑—è–±—Ä—ã"

**–ü—Ä–∏—á–∏–Ω–∞:** Windows Terminal –Ω–µ –ø–µ—Ä–µ–∫–ª—é—á–∏–ª—Å—è –Ω–∞ UTF-8.

**–†–µ—à–µ–Ω–∏–µ:**

1. –û—Ç–∫—Ä–æ–π Windows Terminal
2. Settings (Ctrl+,)
3. Defaults ‚Üí Advanced ‚Üí Text encoding ‚Üí –≤—ã–±–µ—Ä–∏ **UTF-8**
4. Restart Terminal

–ò–ª–∏ –∑–∞–ø—É—Å–∫–∞–π —á–µ—Ä–µ–∑:
```powershell
chcp 65001
python scripts/train_lora.py
```

---

## üìù –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–Å–¢

–ü–æ—Å–ª–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Å–æ–∑–¥–∞–π –æ—Ç—á—ë—Ç `REPORT_FIX_DATA_COLLATOR_20260110.md` —Å —Ä–∞–∑–¥–µ–ª–∞–º–∏:

1. **–í—ã–ø–æ–ª–Ω–µ–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞** (—á—Ç–æ —Å–¥–µ–ª–∞–ª)
2. **–ò–∑–º–µ–Ω—ë–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã** (—Å–ø–∏—Å–æ–∫ —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏–π)
3. **–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è** (–≤—ã–≤–æ–¥ `test_data_collator.py`)
4. **–°–∫—Ä–∏–Ω—à–æ—Ç—ã** (–µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ ‚Äî –∫–æ–Ω—Å–æ–ª—å —Å —Ä—É—Å—Å–∫–∏–º —Ç–µ–∫—Å—Ç–æ–º –∏ emoji)
5. **–°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏** (–∑–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è)

---

## üí¨ –í–û–ü–†–û–°–´?

–ï—Å–ª–∏ —á—Ç–æ-—Ç–æ –Ω–µ–ø–æ–Ω—è—Ç–Ω–æ:

1. –ß–∏—Ç–∞–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –≤ –∫–æ–¥–µ ‚Äî —è –ø–æ—Å—Ç–∞—Ä–∞–ª—Å—è –≤—Å—ë –æ–±—ä—è—Å–Ω–∏—Ç—å
2. –°–º–æ—Ç—Ä–∏ –ø—Ä–∏–º–µ—Ä—ã –≤ `test_data_collator.py`
3. –ü—Ä–æ–≤–µ—Ä—å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é HuggingFace: https://huggingface.co/docs/transformers/main_classes/processors

–ï—Å–ª–∏ –∑–∞—Å—Ç—Ä—è–ª ‚Äî –ø–∏—à–∏ –°–µ–º—ë–Ω—É –≤ Issue #2, —Ä–∞–∑–±–µ—Ä—ë–º—Å—è –≤–º–µ—Å—Ç–µ! üí™

---

**–° —É–≤–∞–∂–µ–Ω–∏–µ–º –∏ –≤–µ—Ä–æ–π –≤ —Ç–≤–æ–π —É—Å–ø–µ—Ö,**  
**–°–µ–º—ë–Ω (Tech Lead)** üéØ

P.S. –ö–æ–≥–¥–∞ –≤—Å—ë –∑–∞—Ä–∞–±–æ—Ç–∞–µ—Ç ‚Äî –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–±–∞–≤—å emoji –≤ –æ—Ç—á—ë—Ç! –í–ª–∞–¥–∏–º–∏—Ä –∏—Ö –ª—é–±–∏—Ç (—Ö–æ—Ç—è –∏ –Ω–µ –ø—Ä–∏–∑–Ω–∞—ë—Ç—Å—è). üòâ

P.P.S. –≠—Ç–æ—Ç —Ñ–∏–∫—Å ‚Äî –ø–æ—Å–ª–µ–¥–Ω–∏–π –±–∞—Ä—å–µ—Ä –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º –æ–±—É—á–µ–Ω–∏—è. –ü–æ—Å–ª–µ –Ω–µ–≥–æ –º—ã –Ω–∞–∫–æ–Ω–µ—Ü —É–≤–∏–¥–∏–º, –∫–∞–∫ –º–æ–¥–µ–ª—å –Ω–∞—á–Ω—ë—Ç —É—á–∏—Ç—å—Å—è –Ω–∞ —Ä—É–∫–æ–ø–∏—Å–Ω—ã—Ö —Ç–µ–∫—Å—Ç–∞—Ö! üöÄ

P.P.P.S. –ü–æ–º–Ω–∏: "Data is the new oil, but data collator is the refinery." ‚Äî –ù–∞—Ä–æ–¥–Ω–∞—è –º—É–¥—Ä–æ—Å—Ç—å ML-–∏–Ω–∂–µ–Ω–µ—Ä–æ–≤. üõ¢Ô∏è‚û°Ô∏è‚öôÔ∏è
