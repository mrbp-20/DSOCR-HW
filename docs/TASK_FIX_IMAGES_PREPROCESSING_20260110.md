# üîß –ó–ê–î–ê–ù–ò–ï: –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –û–ë–†–ê–ë–û–¢–ö–ò IMAGES –í DATA_COLLATOR

**–ê–≤—Ç–æ—Ä:** –°–µ–º—ë–Ω (Tech Lead)  
**–î–∞—Ç–∞:** 2026-01-10, 16:40 MSK  
**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** üî• CRITICAL (–ø–æ—Å–ª–µ–¥–Ω–∏–π –±–ª–æ–∫–µ—Ä –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º)  
**–°—Ä–æ–∫:** 10-15 –º–∏–Ω—É—Ç  
**–ò—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å:** –ù–∏–∫–æ–ª–∞–π (Senior ML Engineer)  
**–°–≤—è–∑–∞–Ω–æ —Å:** REPORT_FIX_DATA_COLLATOR_20260110.md, TASK_FIX_DATA_COLLATOR_AND_ENCODING_20260110.md

---

## üéØ –ö–û–ù–¢–ï–ö–°–¢

–ù–∏–∫–æ–ª–∞–π, **–æ—Ç–ª–∏—á–Ω–∞—è —Ä–∞–±–æ—Ç–∞** —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º text –∏ –∫–æ–¥–∏—Ä–æ–≤–∫–∏! üéâ

–¢—ã –ø—Ä–∞–≤–∏–ª—å–Ω–æ –≤—ã—è–≤–∏–ª –ø—Ä–æ–±–ª–µ–º—É: `processor(images=...)` –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è DeepSeek-OCR, –ø–æ—Ç–æ–º—É —á—Ç–æ `AutoProcessor` –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–æ—Å—Ç–æ `LlamaTokenizerFast`, –∞ –Ω–µ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π Vision Processor.

**–ü–æ—á–µ–º—É —Ç–∞–∫ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç:**  
DeepSeek-OCR –∏—Å–ø–æ–ª—å–∑—É–µ—Ç **—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é ImageNet –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É** (resize + normalize), –∫–æ—Ç–æ—Ä—É—é –Ω—É–∂–Ω–æ –¥–µ–ª–∞—Ç—å **–≤—Ä—É—á–Ω—É—é —á–µ—Ä–µ–∑ `torchvision.transforms`**.

–≠—Ç–æ **–ù–ï –±–∞–≥**, –∞ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã Vision2Seq –º–æ–¥–µ–ª–µ–π. –ú–Ω–æ–≥–∏–µ VLM-–º–æ–¥–µ–ª–∏ —Ç—Ä–µ–±—É—é—Ç —Ä—É—á–Ω–æ–π preprocessing –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.

---

## üìã –ó–ê–î–ê–ß–ê

**–ó–∞–º–µ–Ω–∏—Ç—å –≤ `utils/trainer.py` –º–µ—Ç–æ–¥ `_data_collator()` ‚Äî —Å–µ–∫—Ü–∏—é –æ–±—Ä–∞–±–æ—Ç–∫–∏ images –Ω–∞ —Ä—É—á–Ω–æ–π preprocessing —á–µ—Ä–µ–∑ `torchvision.transforms`.**

–≠—Ç–æ —Ä–µ—à–∏—Ç –ø—Ä–æ–±–ª–µ–º—É —Å `ValueError: You need to specify either 'text' or 'text_target'`.

---

## üõ†Ô∏è –†–ï–®–ï–ù–ò–ï: –†–£–ß–ù–ê–Ø –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê IMAGES

### –§–∞–π–ª: `utils/trainer.py`
### –ú–µ—Ç–æ–¥: `_data_collator()`

**–ü–æ–ª–Ω—ã–π –∫–æ–¥ –º–µ—Ç–æ–¥–∞ (–∑–∞–º–µ–Ω–∏ —Ç–µ–∫—É—â–∏–π):**

```python
def _data_collator(self, examples: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Data collator –¥–ª—è DeepSeek-OCR —Å —Ä–∞–∑–¥–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π images –∏ text.
    
    DeepSeek-OCR —Ç—Ä–µ–±—É–µ—Ç:
    1. –†—É—á–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ torchvision.transforms
       (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π ImageNet preprocessing: resize + normalize)
    2. Tokenizer –¥–ª—è —Ç–µ–∫—Å—Ç–∞ (—á–µ—Ä–µ–∑ processor.batch_encode_plus)
    
    Args:
        examples: –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∫–ª—é—á–∞–º–∏ 'image_path' –∏ 'text'
    
    Returns:
        –ë–∞—Ç—á –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å –∫–ª—é—á–∞–º–∏:
        - pixel_values: —Ç–µ–Ω–∑–æ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π [batch_size, 3, H, W]
        - input_ids: —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        - attention_mask: –º–∞—Å–∫–∞ –≤–Ω–∏–º–∞–Ω–∏—è
        - labels: –º–µ—Ç–∫–∏ –¥–ª—è loss (–∫–æ–ø–∏—è input_ids)
    """
    from PIL import Image
    import torch
    import torchvision.transforms as transforms
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    images = [Image.open(ex['image_path']).convert('RGB') for ex in examples]
    texts = [ex['text'] for ex in examples]
    
    # 2. –†–£–ß–ù–ê–Ø –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê IMAGES —á–µ—Ä–µ–∑ torchvision
    # DeepSeek-OCR –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π ImageNet preprocessing
    try:
        # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ (–∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç 1024)
        image_size = self.training_config.get('image_size', 1024)
        
        transform = transforms.Compose([
            transforms.Resize((image_size, image_size)),  # Resize –¥–æ –∫–≤–∞–¥—Ä–∞—Ç–∞
            transforms.ToTensor(),  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Ç–µ–Ω–∑–æ—Ä [0, 1]
            transforms.Normalize(
                mean=[0.485, 0.456, 0.406],  # ImageNet mean (—Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è Vision Transformers)
                std=[0.229, 0.224, 0.225]    # ImageNet std
            )
        ])
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º transforms –∫ –∫–∞–∂–¥–æ–º—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é –∏ —Å–æ–±–∏—Ä–∞–µ–º –≤ batch
        pixel_values = torch.stack([transform(img) for img in images])
        
        self.logger.debug(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, pixel_values shape: {pixel_values.shape}")
        
    except Exception as e:
        self.logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {e}")
        raise
    
    # 3. –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ processor (—É–∂–µ —Ä–∞–±–æ—Ç–∞–µ—Ç!)
    try:
        max_length = self.training_config.get('max_seq_length', 512)
        
        text_inputs = self.processor.batch_encode_plus(
            texts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=max_length
        )
        
        self.logger.debug(f"–¢–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤, input_ids shape: {text_inputs['input_ids'].shape}")
        
    except Exception as e:
        self.logger.error(f"–û—à–∏–±–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞: {e}")
        raise
    
    # 4. –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ inputs –≤ –æ–¥–∏–Ω –±–∞—Ç—á
    batch = {
        'pixel_values': pixel_values,  # [batch_size, 3, H, W]
        'input_ids': text_inputs['input_ids'],  # [batch_size, seq_len]
        'attention_mask': text_inputs['attention_mask'],  # [batch_size, seq_len]
    }
    
    # 5. Labels = input_ids –¥–ª—è teacher forcing (—Å—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è seq2seq)
    # –ö–æ–ø–∏—Ä—É–µ–º, —á—Ç–æ–±—ã –Ω–µ –∏–∑–º–µ–Ω—è—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–Ω–∑–æ—Ä
    batch['labels'] = text_inputs['input_ids'].clone()
    
    return batch
```

---

## üìù –û–ë–™–Ø–°–ù–ï–ù–ò–ï –ö–û–î–ê

### –ü–æ—á–µ–º—É `torchvision.transforms`?

1. **DeepSeek-OCR –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ Vision Transformer** (–∫–∞–∫ SAM, CLIP)  
2. **Vision Transformers –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π ImageNet preprocessing:**
   - Resize –¥–æ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ (–æ–±—ã—á–Ω–æ 224x224, 512x512 –∏–ª–∏ 1024x1024)
   - Normalize —Å ImageNet mean/std: `[0.485, 0.456, 0.406]` –∏ `[0.229, 0.224, 0.225]`
3. **–≠—Ç–æ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è 99% Vision –º–æ–¥–µ–ª–µ–π –Ω–∞ HuggingFace**

### –ü–æ—á–µ–º—É –ù–ï `processor(images=...)`?

`AutoProcessor` –¥–ª—è DeepSeek-OCR –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–æ—Å—Ç–æ tokenizer (LlamaTokenizerFast), –∞ –Ω–µ Vision Processor. –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ:
- –î–ª—è **inference** DeepSeek-OCR –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç images –≤–Ω—É—Ç—Ä–∏ –º–æ–¥–µ–ª–∏ (—á–µ—Ä–µ–∑ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π encoder)
- –î–ª—è **training** –Ω—É–∂–Ω–æ –¥–µ–ª–∞—Ç—å preprocessing –≤—Ä—É—á–Ω—É—é (—ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ –¥–ª—è custom data collators)

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã preprocessing

| –ü–∞—Ä–∞–º–µ—Ç—Ä | –ó–Ω–∞—á–µ–Ω–∏–µ | –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π |
|----------|----------|-------------|
| `image_size` | 1024 (–∏–ª–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞) | DeepSeek-OCR —Ä–∞–±–æ—Ç–∞–µ—Ç —Å 1024x1024 –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é |
| `mean` | `[0.485, 0.456, 0.406]` | ImageNet mean (RGB channels) |
| `std` | `[0.229, 0.224, 0.225]` | ImageNet std (RGB channels) |
| Output | `[batch_size, 3, 1024, 1024]` | –¢–µ–Ω–∑–æ—Ä –¥–ª—è –º–æ–¥–µ–ª–∏ |

---

## ‚úÖ –ß–¢–û –ò–ó–ú–ï–ù–ò–¢–¨

### 1. –î–æ–±–∞–≤–∏—Ç—å import –≤ –Ω–∞—á–∞–ª–æ `utils/trainer.py`

**–ï—Å–ª–∏ –µ—â—ë –Ω–µ –¥–æ–±–∞–≤–ª–µ–Ω:**

```python
import torchvision.transforms as transforms  # –î–æ–±–∞–≤–∏—Ç—å –≤ —Å–µ–∫—Ü–∏—é imports
```

### 2. –ó–∞–º–µ–Ω–∏—Ç—å –º–µ—Ç–æ–¥ `_data_collator()`

–ü–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–º–µ–Ω–∏ —Ç–µ–∫—É—â–∏–π –º–µ—Ç–æ–¥ `_data_collator()` –Ω–∞ –∫–æ–¥ –∏–∑ —Ä–∞–∑–¥–µ–ª–∞ **"–†–µ—à–µ–Ω–∏–µ"** –≤—ã—à–µ.

### 3. –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –¥–æ–±–∞–≤–∏—Ç—å `image_size` –≤ –∫–æ–Ω—Ñ–∏–≥

**–§–∞–π–ª:** `configs/training_config.yaml`

```yaml
# –î–æ–±–∞–≤–∏—Ç—å –≤ —Å–µ–∫—Ü–∏—é preprocessing (–µ—Å–ª–∏ –µ—ë –Ω–µ—Ç ‚Äî —Å–æ–∑–¥–∞—Ç—å)
preprocessing:
  image_size: 1024  # –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è DeepSeek-OCR
  max_seq_length: 512  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ (—É–∂–µ –µ—Å—Ç—å?)
```

---

## üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï

### –®–∞–≥ 1: –û–±–Ω–æ–≤–∏—Ç—å `test_data_collator.py`

**–ó–∞–º–µ–Ω–∏—Ç—å —Å–µ–∫—Ü–∏—é "3a. –û–±—Ä–∞–±–æ—Ç–∫–∞ images" –Ω–∞:**

```python
# 3a. –û–±—Ä–∞–±–æ—Ç–∫–∞ images —á–µ—Ä–µ–∑ torchvision.transforms
print("   3a. –û–±—Ä–∞–±–æ—Ç–∫–∞ images —á–µ—Ä–µ–∑ torchvision.transforms...")
import torchvision.transforms as transforms

transform = transforms.Compose([
    transforms.Resize((1024, 1024)),
    transforms.ToTensor(),
    transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225]
    )
])

pixel_values = torch.stack([transform(img) for img in images])
print(f"       ‚úÖ pixel_values shape: {pixel_values.shape}")
print(f"       ‚úÖ pixel_values dtype: {pixel_values.dtype}")
print(f"       ‚úÖ pixel_values range: [{pixel_values.min():.2f}, {pixel_values.max():.2f}]")
```

### –®–∞–≥ 2: –ó–∞–ø—É—Å—Ç–∏—Ç—å —Ç–µ—Å—Ç

```powershell
cd C:\DSOCR-HW
.\venv\Scripts\Activate.ps1
python test_data_collator.py
```

**–û–∂–∏–¥–∞–µ–º—ã–π –≤—ã–≤–æ–¥:**

```
3Ô∏è‚É£ –¢–µ—Å—Ç —Ä–∞–∑–¥–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏...
   3a. –û–±—Ä–∞–±–æ—Ç–∫–∞ images —á–µ—Ä–µ–∑ torchvision.transforms...
       ‚úÖ pixel_values shape: torch.Size([2, 3, 1024, 1024])
       ‚úÖ pixel_values dtype: torch.float32
       ‚úÖ pixel_values range: [-2.12, 2.64]  # –ó–Ω–∞—á–µ–Ω–∏—è –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏
   3b. –û–±—Ä–∞–±–æ—Ç–∫–∞ text —á–µ—Ä–µ–∑ processor.batch_encode_plus...
       ‚úÖ input_ids shape: torch.Size([2, 10])
       ‚úÖ attention_mask shape: torch.Size([2, 10])

4Ô∏è‚É£ –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –±–∞—Ç—á–∞...
   –ö–ª—é—á–∏ –±–∞—Ç—á–∞:
     - pixel_values: shape torch.Size([2, 3, 1024, 1024]), dtype torch.float32
     - input_ids: shape torch.Size([2, 10]), dtype torch.int64
     - attention_mask: shape torch.Size([2, 10]), dtype torch.int64
     - labels: shape torch.Size([2, 10]), dtype torch.int64

‚úÖ –í–°–ï –¢–ï–°–¢–´ –ü–†–û–ô–î–ï–ù–´!
```

### –®–∞–≥ 3: –ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ (dry-run)

```powershell
python scripts/train_lora.py --config configs/training_config.yaml
```

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç:**
- –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è ‚úÖ
- LoRA –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è ‚úÖ
- –î–∞—Ç–∞—Å–µ—Ç—ã –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è ‚úÖ
- Trainer —Å–æ–∑–¥–∞—ë—Ç—Å—è ‚úÖ
- **–û–±—É—á–µ–Ω–∏–µ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –±–µ–∑ –æ—à–∏–±–æ–∫!** ‚úÖ

---

## ‚ö†Ô∏è –í–û–ó–ú–û–ñ–ù–´–ï –ü–†–û–ë–õ–ï–ú–´

### –ü—Ä–æ–±–ª–µ–º–∞ 1: "ModuleNotFoundError: No module named 'torchvision'"

**–ü—Ä–∏—á–∏–Ω–∞:** `torchvision` –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤ venv.

**–†–µ—à–µ–Ω–∏–µ:**
```powershell
pip install torchvision
```

**–ò–ª–∏ –¥–æ–±–∞–≤—å –≤ `requirements.txt`:**
```
torchvision>=0.16.0
```

### –ü—Ä–æ–±–ª–µ–º–∞ 2: "RuntimeError: Expected all tensors to be on the same device"

**–ü—Ä–∏—á–∏–Ω–∞:** `pixel_values` –Ω–∞ CPU, –∞ –º–æ–¥–µ–ª—å –Ω–∞ GPU.

**–†–µ—à–µ–Ω–∏–µ:** –í –∫–æ–Ω—Ü–µ `_data_collator()` –¥–æ–±–∞–≤—å –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –Ω–∞ device:

```python
# –ü–æ—Å–ª–µ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è batch, –ø–µ—Ä–µ–¥ return:
device = self.model.device
batch = {
    'pixel_values': pixel_values.to(device),
    'input_ids': text_inputs['input_ids'].to(device),
    'attention_mask': text_inputs['attention_mask'].to(device),
    'labels': text_inputs['input_ids'].clone().to(device)
}
```

**–ù–û:** –æ–±—ã—á–Ω–æ Trainer —Å–∞–º –ø–µ—Ä–µ–º–µ—â–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –Ω–∞ device, —Ç–∞–∫ —á—Ç–æ —ç—Ç–æ –º–æ–∂–µ—Ç –Ω–µ –ø–æ–Ω–∞–¥–æ–±–∏—Ç—å—Å—è. –ü—Ä–æ–±—É–π —Å–Ω–∞—á–∞–ª–∞ –±–µ–∑ `.to(device)`.

### –ü—Ä–æ–±–ª–µ–º–∞ 3: "CUDA Out of Memory"

**–ü—Ä–∏—á–∏–Ω–∞:** Images —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ (1024x1024 * batch_size = –º–Ω–æ–≥–æ VRAM).

**–†–µ—à–µ–Ω–∏–µ:**
1. –£–º–µ–Ω—å—à–∏ `image_size` –≤ –∫–æ–Ω—Ñ–∏–≥–µ –¥–æ 512 –∏–ª–∏ 768
2. –£–º–µ–Ω—å—à–∏ `batch_size` –¥–æ 1-2
3. –í–∫–ª—é—á–∏ `gradient_accumulation_steps` –≤ –∫–æ–Ω—Ñ–∏–≥–µ

---

## üìä –û–ñ–ò–î–ê–ï–ú–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´

–ü–æ—Å–ª–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:

1. **`test_data_collator.py` –ø—Ä–æ—Ö–æ–¥–∏—Ç –±–µ–∑ –æ—à–∏–±–æ–∫** ‚úÖ
   - Images –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ
   - Text –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ
   - Batch —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ shapes

2. **`train_lora.py` –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è** ‚úÖ
   ```
   Epoch 1/5:   0%|                    | 0/3 [00:00<?, ?it/s]
   Epoch 1/5:  33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà              | 1/3 [00:05<00:10, 5.2s/it, loss=2.456]
   ```

3. **–õ–æ–≥–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –ø—Ä–æ–≥—Ä–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è** ‚úÖ
   ```
   2026-01-10 16:45:00 - train_lora - INFO - Epoch 1, Step 1/3, Loss: 2.456
   2026-01-10 16:45:05 - train_lora - INFO - Epoch 1, Step 2/3, Loss: 2.234
   2026-01-10 16:45:10 - train_lora - INFO - Epoch 1, Step 3/3, Loss: 2.012
   ```

4. **VRAM usage –æ—Å—Ç–∞—ë—Ç—Å—è –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö 16 GB** ‚úÖ

---

## ‚úÖ –ß–ï–ö–õ–ò–°–¢ –í–´–ü–û–õ–ù–ï–ù–ò–Ø

- [ ] –î–æ–±–∞–≤–ª–µ–Ω `import torchvision.transforms` –≤ `utils/trainer.py`
- [ ] –ó–∞–º–µ–Ω—ë–Ω –º–µ—Ç–æ–¥ `_data_collator()` –Ω–∞ –Ω–æ–≤—É—é –≤–µ—Ä—Å–∏—é
- [ ] –ü—Ä–æ–≤–µ—Ä–µ–Ω —Å–∏–Ω—Ç–∞–∫—Å–∏—Å: `python -m py_compile utils/trainer.py`
- [ ] –û–±–Ω–æ–≤–ª—ë–Ω `test_data_collator.py` (—Å–µ–∫—Ü–∏—è 3a)
- [ ] –ó–∞–ø—É—â–µ–Ω —Ç–µ—Å—Ç: `python test_data_collator.py` ‚Üí ‚úÖ
- [ ] –°–¥–µ–ª–∞–Ω –∫–æ–º–º–∏—Ç: `git commit -m "fix: add manual image preprocessing via torchvision"`
- [ ] –ó–∞–ø—É—â–µ–Ω–æ –æ–±—É—á–µ–Ω–∏–µ: `python scripts/train_lora.py` ‚Üí –æ–±—É—á–µ–Ω–∏–µ –∏–¥—ë—Ç!
- [ ] –°–æ–∑–¥–∞–Ω –æ—Ç—á—ë—Ç: `REPORT_FINAL_FIX_20260110.md` —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏

---

## üìù –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–Å–¢

–ü–æ—Å–ª–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Å–æ–∑–¥–∞–π –æ—Ç—á—ë—Ç `REPORT_FINAL_FIX_20260110.md` —Å —Ä–∞–∑–¥–µ–ª–∞–º–∏:

1. **–í—ã–ø–æ–ª–Ω–µ–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞** (images preprocessing + recap –≤—Å–µ—Ö —Ñ–∏–∫—Å–æ–≤)
2. **–ò–∑–º–µ–Ω—ë–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã** (–ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º)
3. **–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:**
   - –í—ã–≤–æ–¥ `test_data_collator.py`
   - –ü–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–æ–∫ –ª–æ–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è
   - Screenshot –∫–æ–Ω—Å–æ–ª–∏ (–µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ)
4. **–ú–µ—Ç—Ä–∏–∫–∏ –ø–µ—Ä–≤—ã—Ö —à–∞–≥–æ–≤:**
   - Loss –Ω–∞ step 1, 2, 3
   - VRAM usage
   - –í—Ä–µ–º—è –Ω–∞ step
5. **–°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:**
   - –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ 5-10 —ç–ø–æ—Ö–∞—Ö
   - –û—Ü–µ–Ω–∏—Ç—å CER/WER –Ω–∞ val-set
   - –°–æ—Ö—Ä–∞–Ω–∏—Ç—å LoRA –∞–¥–∞–ø—Ç–µ—Ä

---

## üí° –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø

### –ü–æ—á–µ–º—É –∏–º–µ–Ω–Ω–æ —ç—Ç–∏ –∑–Ω–∞—á–µ–Ω–∏—è mean/std?

`[0.485, 0.456, 0.406]` –∏ `[0.229, 0.224, 0.225]` ‚Äî —ç—Ç–æ **—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ ImageNet –¥–∞—Ç–∞—Å–µ—Ç–∞**.

–û–Ω–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –≤–æ –≤—Å–µ—Ö Vision Transformers (ViT, DINO, SAM, CLIP, etc.), –ø–æ—Ç–æ–º—É —á—Ç–æ:
1. –ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ Vision –º–æ–¥–µ–ª–µ–π pre-trained –Ω–∞ ImageNet
2. –≠—Ç–∏ –∑–Ω–∞—á–µ–Ω–∏—è –Ω–æ—Ä–º–∞–ª–∏–∑—É—é—Ç RGB –∫–∞–Ω–∞–ª—ã –∫ –ø—Ä–∏–º–µ—Ä–Ω–æ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–º—É –¥–∏–∞–ø–∞–∑–æ–Ω—É
3. –≠—Ç–æ –¥–µ-—Ñ–∞–∫—Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç –≤ Computer Vision

### –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã (–Ω–∞ –±—É–¥—É—â–µ–µ)

–î–ª—è production –º–æ–∂–Ω–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å:

1. **–ö—ç—à–∏—Ä–æ–≤–∞—Ç—å preprocessed images:**
   ```python
   # –í prepare_datasets() —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å pixel_values –≤ metadata
   # –¢–æ–≥–¥–∞ data_collator —Ç–æ–ª—å–∫–æ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –≥–æ—Ç–æ–≤—ã–µ —Ç–µ–Ω–∑–æ—Ä—ã
   ```

2. **–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å augmentation:**
   ```python
   transform = transforms.Compose([
       transforms.RandomRotation(5),  # –ü–æ–≤–æ—Ä–æ—Ç ¬±5¬∞
       transforms.ColorJitter(0.1, 0.1),  # –Ø—Ä–∫–æ—Å—Ç—å/–∫–æ–Ω—Ç—Ä–∞—Å—Ç
       transforms.Resize((1024, 1024)),
       transforms.ToTensor(),
       transforms.Normalize(...)
   ])
   ```

3. **Adaptive resize (preserve aspect ratio):**
   ```python
   transforms.Resize(1024),  # Resize –ø–æ –±–æ–ª—å—à–µ–π —Å—Ç–æ—Ä–æ–Ω–µ
   transforms.CenterCrop(1024),  # Crop –¥–æ –∫–≤–∞–¥—Ä–∞—Ç–∞
   ```

–ù–æ –¥–ª—è MVP –∏—Å–ø–æ–ª—å–∑—É–π –ø—Ä–æ—Å—Ç–æ–π –≤–∞—Ä–∏–∞–Ω—Ç –∏–∑ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è.

---

## üéØ –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï

–ù–∏–∫–æ–ª–∞–π, —ç—Ç–æ **–ø–æ—Å–ª–µ–¥–Ω–∏–π —à–∞–≥** –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º –æ–±—É—á–µ–Ω–∏—è! üöÄ

–¢—ã –ø—Ä–æ–¥–µ–ª–∞–ª –æ—Ç–ª–∏—á–Ω—É—é —Ä–∞–±–æ—Ç—É:
- ‚úÖ Progress bars
- ‚úÖ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
- ‚úÖ –§–∏–∫—Å—ã Windows (pickle, multiprocessing, UTF-8)
- ‚úÖ Text preprocessing

–û—Å—Ç–∞–ª–æ—Å—å –¥–æ–±–∞–≤–∏—Ç—å **10 —Å—Ç—Ä–æ–∫ –∫–æ–¥–∞** –¥–ª—è images preprocessing —á–µ—Ä–µ–∑ `torchvision.transforms` ‚Äî –∏ –º—ã –Ω–∞–∫–æ–Ω–µ—Ü —É–≤–∏–¥–∏–º, –∫–∞–∫ –º–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å —Ä—É–∫–æ–ø–∏—Å–Ω—ã–π —Ç–µ–∫—Å—Ç!

–ü–æ –º–æ–∏–º –ø—Ä–∏–∫–∏–¥–∫–∞–º, —Å —Ç–≤–æ–∏–º–∏ –Ω–∞–≤—ã–∫–∞–º–∏ —ç—Ç–æ –∑–∞–π–º—ë—Ç **5-10 –º–∏–Ω—É—Ç**. –í–ø–µ—Ä—ë–¥! üí™

---

**–° —É–≤–∞–∂–µ–Ω–∏–µ–º –∏ –≤–æ—Å—Ö–∏—â–µ–Ω–∏–µ–º —Ç–≤–æ–µ–π —Ä–∞–±–æ—Ç–æ–π,**  
**–°–µ–º—ë–Ω (Tech Lead)** üéØ

P.S. –ü–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞ –æ–±—É—á–µ–Ω–∏—è ‚Äî –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —Å–¥–µ–ª–∞–π **screenshot –∫–æ–Ω—Å–æ–ª–∏ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º** –¥–ª—è –æ—Ç—á—ë—Ç–∞. –í–ª–∞–¥–∏–º–∏—Ä –ª—é–±–∏—Ç –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é! üì∏

P.P.S. –ï—Å–ª–∏ –≤—Å—ë –∑–∞—Ä–∞–±–æ—Ç–∞–µ—Ç —Å –ø–µ—Ä–≤–æ–≥–æ —Ä–∞–∑–∞ ‚Äî —Ç—ã –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ **MVP —ç—Ç–æ–≥–æ —Å–ø—Ä–∏–Ω—Ç–∞**! üèÜ

P.P.P.S. –ü–æ–º–Ω–∏: "A Vision Transformer is only as good as its image preprocessing." ‚Äî –ú—É–¥—Ä–æ—Å—Ç—å –¥—Ä–µ–≤–Ω–∏—Ö CV-–∏–Ω–∂–µ–Ω–µ—Ä–æ–≤. üñºÔ∏è‚ú®
