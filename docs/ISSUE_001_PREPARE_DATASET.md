# Issue #1: –†–µ–∞–ª–∏–∑–∞—Ü–∏—è prepare_dataset.py

**–û—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω—ã–π**: –ù–∏–∫–æ–ª–∞–π (Cursor AI)  
**–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è**: 08.01.2026  
**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç**: High  
**–°—Ç–∞—Ç—É—Å**: Open  
**–¢–µ–≥–∏**: `data-processing`, `oop`, `module`, `issue-001`

---

## üéØ –¶–µ–ª—å –∑–∞–¥–∞—á–∏

–°–æ–∑–¥–∞—Ç—å –º–æ–¥—É–ª—å–Ω—ã–π, –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π —Å–∫—Ä–∏–ø—Ç `prepare_dataset.py` —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º **–û–û–ü-–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã** –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Ä—É–∫–æ–ø–∏—Å–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –∫ fine-tuning DeepSeek-OCR.

**–ö–ª—é—á–µ–≤—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è**:
- ‚úÖ **–ú–æ–¥—É–ª—å–Ω–æ—Å—Ç—å**: –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã (–∑–∞–≥—Ä—É–∑–∫–∞, –≤–∞–ª–∏–¥–∞—Ü–∏—è, –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞, split) ‚Äî –æ—Ç–¥–µ–ª—å–Ω—ã–µ –∫–ª–∞—Å—Å—ã
- ‚úÖ **–ü–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º–æ—Å—Ç—å**: –∫–ª–∞—Å—Å—ã –¥–æ–ª–∂–Ω—ã —Ä–∞–±–æ—Ç–∞—Ç—å –≤ –ª—é–±–æ–º –ø—Ä–æ–µ–∫—Ç–µ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏
- ‚úÖ **–û–û–ü**: –∏–Ω–∫–∞–ø—Å—É–ª—è—Ü–∏—è, –Ω–∞—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ, –ø–æ–ª–∏–º–æ—Ä—Ñ–∏–∑–º
- ‚úÖ **Type hints**: –¥–ª—è –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤ –∏ –∞—Ç—Ä–∏–±—É—Ç–æ–≤
- ‚úÖ **Docstrings**: Google style –¥–ª—è –≤—Å–µ—Ö –∫–ª–∞—Å—Å–æ–≤ –∏ –º–µ—Ç–æ–¥–æ–≤
- ‚úÖ **–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ**: —á–µ—Ä–µ–∑ `utils/logger.py`
- ‚úÖ **–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫**: try-except —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º traceback

---

## üì¶ –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Ä–µ—à–µ–Ω–∏—è

### –ú–æ–¥—É–ª—å–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞

```
utils/
‚îú‚îÄ‚îÄ data_loader.py           # –ö–ª–∞—Å—Å DatasetLoader
‚îú‚îÄ‚îÄ data_validator.py        # –ö–ª–∞—Å—Å DataValidator
‚îú‚îÄ‚îÄ image_processor.py       # –ö–ª–∞—Å—Å ImageProcessor (—É–∂–µ –µ—Å—Ç—å, —Ä–∞—Å—à–∏—Ä–∏—Ç—å)
‚îú‚îÄ‚îÄ dataset_splitter.py      # –ö–ª–∞—Å—Å DatasetSplitter
‚îî‚îÄ‚îÄ logger.py                # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è (—É–∂–µ –µ—Å—Ç—å)

scripts/
‚îî‚îÄ‚îÄ prepare_dataset.py       # –ì–ª–∞–≤–Ω—ã–π —Å–∫—Ä–∏–ø—Ç (–æ—Ä–∫–µ—Å—Ç—Ä–∏—Ä—É–µ—Ç –∫–ª–∞—Å—Å—ã)
```

### –ü—Ä–∏–Ω—Ü–∏–ø—ã –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è

1. **Single Responsibility Principle (SRP)**: –∫–∞–∂–¥—ã–π –∫–ª–∞—Å—Å –æ—Ç–≤–µ—á–∞–µ—Ç –∑–∞ –æ–¥–Ω—É –∑–∞–¥–∞—á—É
2. **Open/Closed Principle (OCP)**: –∫–ª–∞—Å—Å—ã –æ—Ç–∫—Ä—ã—Ç—ã –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è, –∑–∞–∫—Ä—ã—Ç—ã –¥–ª—è –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏
3. **Dependency Injection**: –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–µ—Ä–µ–¥–∞—ë—Ç—Å—è —á–µ—Ä–µ–∑ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä
4. **Strategy Pattern**: —Ä–∞–∑–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π

---

## üìã –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è

### –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ

**1. –°—ã—Ä—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è**: `data/raw/`
- –§–æ—Ä–º–∞—Ç—ã: JPG, PNG, TIFF
- –õ—é–±–æ–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ
- RGB –∏–ª–∏ Grayscale

**2. –ê–Ω–Ω–æ—Ç–∞—Ü–∏–∏**: `data/annotated/annotations.csv`

**–§–æ—Ä–º–∞—Ç CSV** (UTF-8, —Å –∫–∞–≤—ã—á–∫–∞–º–∏ –¥–ª—è –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞):
```csv
filename,text
handwriting_001.jpg,"–í–æ–ø—Ä–æ—Å –æ —Ç–æ–º - —Å–º–æ–∂–µ–º –ª–∏ –º—ã —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å
—Å–∏—Å—Ç–µ–º—É –¥–æ–æ–±—É—á–µ–Ω–∏—è —Ä—É–∫–æ–ø–∏—Å–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É
–≤ –Ω–∞—Å—Ç–æ—è—â–∏–π –º–æ–º–µ–Ω—Ç –Ω–µ –∏–º–µ–µ—Ç –æ—Ç–≤–µ—Ç–∞."
handwriting_002.png,"–î—Ä—É–≥–æ–π –ø—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞."
```

**–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è**:
- `filename` (str): –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ `data/raw/`
- `text` (str): ground truth —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è (–º–æ–∂–µ—Ç –±—ã—Ç—å –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω–æ–π)

**–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –ø–æ–ª—è** (–¥–ª—è –±—É–¥—É—â–µ–≥–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è):
- `language` (str): –∫–æ–¥ —è–∑—ã–∫–∞ (ru, en, ru-en)
- `quality` (str): –∫–∞—á–µ—Å—Ç–≤–æ —Å–∫–∞–Ω–∞ (excellent, good, medium, poor)
- `notes` (str): –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏

### –í—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ

**–°—Ç—Ä—É–∫—Ç—É—Ä–∞**:
```
data/processed/
‚îú‚îÄ‚îÄ train/
‚îÇ   ‚îú‚îÄ‚îÄ images/                 # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 00001.jpg
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 00002.jpg
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ metadata.json          # –ê–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –¥–ª—è train
‚îî‚îÄ‚îÄ val/
    ‚îú‚îÄ‚îÄ images/
    ‚îÇ   ‚îú‚îÄ‚îÄ 00001.jpg
    ‚îÇ   ‚îî‚îÄ‚îÄ ...
    ‚îî‚îÄ‚îÄ metadata.json          # –ê–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –¥–ª—è val
```

**–§–æ—Ä–º–∞—Ç metadata.json**:
```json
[
  {
    "image_id": "00001",
    "image_path": "images/00001.jpg",
    "text": "–¢–µ–∫—Å—Ç —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n–ú–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã–π",
    "original_filename": "handwriting_001.jpg",
    "width": 1024,
    "height": 768,
    "split": "train"
  },
  ...
]
```

### Split –ø–∞—Ä–∞–º–µ—Ç—Ä—ã

- **Train**: 80% –æ–±—Ä–∞–∑—Ü–æ–≤
- **Val**: 20% –æ–±—Ä–∞–∑—Ü–æ–≤
- **–°—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è**: –ø–æ –¥–ª–∏–Ω–µ —Ç–µ–∫—Å—Ç–∞ (–∫–æ—Ä–æ—Ç–∫–∏–µ/—Å—Ä–µ–¥–Ω–∏–µ/–¥–ª–∏–Ω–Ω—ã–µ) ‚Äî –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ
- **Random seed**: 42 (–¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏)

### –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π

**–ë–∞–∑–æ–≤–∞—è** (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è):
1. **Resize**: max dimension = 1024px (—Å–æ—Ö—Ä–∞–Ω–∏—Ç—å aspect ratio)
2. **Normalize**: RGB ‚Üí [0, 1]
3. **Format**: —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–∞–∫ JPG (quality=95)

**–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è** (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –¥–ª—è –±—É–¥—É—â–µ–≥–æ):
4. **Augmentation** (—Ç–æ–ª—å–∫–æ –¥–ª—è train):
   - –ü–æ–≤–æ—Ä–æ—Ç: ¬±5¬∞
   - –Ø—Ä–∫–æ—Å—Ç—å: ¬±10%
   - –ö–æ–Ω—Ç—Ä–∞—Å—Ç: ¬±10%
5. **Grayscale** ‚Üí RGB (–µ—Å–ª–∏ –∏—Å—Ö–æ–¥–Ω–∏–∫ grayscale)
6. **Binarization** (–¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∞)

---

## üèóÔ∏è –î–µ—Ç–∞–ª—å–Ω–∞—è —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–ª–∞—Å—Å–æ–≤

### 1. –ö–ª–∞—Å—Å `DatasetLoader` (utils/data_loader.py)

**–û—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å**: –ó–∞–≥—Ä—É–∑–∫–∞ CSV-–∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π

```python
from pathlib import Path
from typing import List, Dict, Tuple
import pandas as pd
from PIL import Image


class DatasetLoader:
    """–ó–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ CSV –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π.
    
    Attributes:
        annotations_path: –ü—É—Ç—å –∫ CSV-—Ñ–∞–π–ª—É —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏
        images_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å —Å—ã—Ä—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
    """
    
    def __init__(self, annotations_path: Path, images_dir: Path):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≥—Ä—É–∑—á–∏–∫–∞.
        
        Args:
            annotations_path: –ü—É—Ç—å –∫ annotations.csv
            images_dir: –ü—É—Ç—å –∫ data/raw/
        
        Raises:
            FileNotFoundError: –ï—Å–ª–∏ —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã
        """
        self.annotations_path = annotations_path
        self.images_dir = images_dir
        self._validate_paths()
    
    def _validate_paths(self) -> None:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –ø—É—Ç–µ–π."""
        if not self.annotations_path.exists():
            raise FileNotFoundError(f"Annotations file not found: {self.annotations_path}")
        if not self.images_dir.exists():
            raise FileNotFoundError(f"Images directory not found: {self.images_dir}")
    
    def load_annotations(self) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –∏–∑ CSV.
        
        Returns:
            DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏: filename, text, [language, quality, notes]
        
        Raises:
            ValueError: –ï—Å–ª–∏ CSV –∏–º–µ–µ—Ç –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
        """
        try:
            df = pd.read_csv(self.annotations_path, encoding='utf-8')
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
            required_columns = ['filename', 'text']
            if not all(col in df.columns for col in required_columns):
                raise ValueError(f"CSV must contain columns: {required_columns}")
            
            # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ —Å –ø—É—Å—Ç—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
            df = df.dropna(subset=required_columns)
            
            return df
            
        except Exception as e:
            raise ValueError(f"Failed to load annotations: {e}")
    
    def load_image(self, filename: str) -> Image.Image:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.
        
        Args:
            filename: –ò–º—è —Ñ–∞–π–ª–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'handwriting_001.jpg')
        
        Returns:
            PIL Image –æ–±—ä–µ–∫—Ç
        
        Raises:
            FileNotFoundError: –ï—Å–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ
        """
        image_path = self.images_dir / filename
        if not image_path.exists():
            raise FileNotFoundError(f"Image not found: {image_path}")
        
        return Image.open(image_path).convert('RGB')
    
    def load_dataset(self) -> List[Dict[str, any]]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–ª–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ (–∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ + –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è).
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∫–ª—é—á–∞–º–∏:
                - 'filename': str
                - 'text': str
                - 'image': PIL.Image
                - 'width': int
                - 'height': int
        
        Raises:
            FileNotFoundError: –ï—Å–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç
        """
        annotations = self.load_annotations()
        dataset = []
        
        for idx, row in annotations.iterrows():
            filename = row['filename']
            text = row['text']
            
            try:
                image = self.load_image(filename)
                width, height = image.size
                
                dataset.append({
                    'filename': filename,
                    'text': text,
                    'image': image,
                    'width': width,
                    'height': height
                })
            except FileNotFoundError as e:
                # –õ–æ–≥–∏—Ä—É–µ–º –∏ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                print(f"Warning: {e}")
                continue
        
        return dataset
```

---

### 2. –ö–ª–∞—Å—Å `DataValidator` (utils/data_validator.py)

**–û—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å**: –í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

```python
from typing import List, Dict
import re


class DataValidator:
    """–í–∞–ª–∏–¥–∞—Ç–æ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞.
    
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç:
    - –ù–∞–ª–∏—á–∏–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
    - –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Ñ–æ—Ä–º–∞—Ç–æ–≤
    - –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    - –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π
    """
    
    def __init__(self, min_image_size: int = 100, max_text_length: int = 10000):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–ª–∏–¥–∞—Ç–æ—Ä–∞.
        
        Args:
            min_image_size: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–ø–∏–∫—Å–µ–ª–∏)
            max_text_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ (—Å–∏–º–≤–æ–ª—ã)
        """
        self.min_image_size = min_image_size
        self.max_text_length = max_text_length
    
    def validate_sample(self, sample: Dict[str, any]) -> Tuple[bool, str]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –æ–¥–Ω–æ–≥–æ –æ–±—Ä–∞–∑—Ü–∞.
        
        Args:
            sample: –°–ª–æ–≤–∞—Ä—å —Å –∫–ª—é—á–∞–º–∏ 'filename', 'text', 'image', 'width', 'height'
        
        Returns:
            Tuple (is_valid, error_message)
        """
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
        required_fields = ['filename', 'text', 'image', 'width', 'height']
        for field in required_fields:
            if field not in sample:
                return False, f"Missing field: {field}"
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        if sample['width'] < self.min_image_size or sample['height'] < self.min_image_size:
            return False, f"Image too small: {sample['width']}x{sample['height']}"
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ–∫—Å—Ç–∞
        text = sample['text'].strip()
        if len(text) == 0:
            return False, "Empty text"
        if len(text) > self.max_text_length:
            return False, f"Text too long: {len(text)} chars"
        
        return True, ""
    
    def validate_dataset(self, dataset: List[Dict[str, any]]) -> List[Dict[str, any]]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Å–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞.
        
        Args:
            dataset: –°–ø–∏—Å–æ–∫ –æ–±—Ä–∞–∑—Ü–æ–≤
        
        Returns:
            –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –≤–∞–ª–∏–¥–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤
        
        Raises:
            ValueError: –ï—Å–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç –ø—É—Å—Ç–æ–π –ø–æ—Å–ª–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        """
        valid_samples = []
        invalid_count = 0
        
        for sample in dataset:
            is_valid, error = self.validate_sample(sample)
            if is_valid:
                valid_samples.append(sample)
            else:
                print(f"Warning: Invalid sample {sample['filename']}: {error}")
                invalid_count += 1
        
        if len(valid_samples) == 0:
            raise ValueError("No valid samples found in dataset")
        
        print(f"Validation complete: {len(valid_samples)} valid, {invalid_count} invalid")
        return valid_samples
```

---

### 3. –ö–ª–∞—Å—Å `ImageProcessor` (utils/image_processor.py)

**–û—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å**: –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π

```python
from pathlib import Path
from typing import Tuple
from PIL import Image
import numpy as np


class ImageProcessor:
    """–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏.
    
    Attributes:
        max_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –ø–æ –±–æ–ª—å—à–µ–π —Å—Ç–æ—Ä–æ–Ω–µ
        jpeg_quality: –ö–∞—á–µ—Å—Ç–≤–æ JPEG –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏
    """
    
    def __init__(self, max_size: int = 1024, jpeg_quality: int = 95):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞.
        
        Args:
            max_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–ø–∏–∫—Å–µ–ª–∏)
            jpeg_quality: –ö–∞—á–µ—Å—Ç–≤–æ JPEG (1-100)
        """
        self.max_size = max_size
        self.jpeg_quality = jpeg_quality
    
    def resize_keep_aspect_ratio(self, image: Image.Image) -> Image.Image:
        """Resize —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º aspect ratio.
        
        Args:
            image: –ò—Å—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        
        Returns:
            Resized –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        """
        width, height = image.size
        
        # –ï—Å–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –º–µ–Ω—å—à–µ max_size, –Ω–µ –º–µ–Ω—è–µ–º
        if max(width, height) <= self.max_size:
            return image
        
        # –í—ã—á–∏—Å–ª—è–µ–º –Ω–æ–≤—ã–π —Ä–∞–∑–º–µ—Ä
        if width > height:
            new_width = self.max_size
            new_height = int(height * (self.max_size / width))
        else:
            new_height = self.max_size
            new_width = int(width * (self.max_size / height))
        
        return image.resize((new_width, new_height), Image.Resampling.LANCZOS)
    
    def normalize(self, image: Image.Image) -> np.ndarray:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤ [0, 1].
        
        Args:
            image: PIL Image
        
        Returns:
            Numpy array (H, W, 3) —Å –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ [0, 1]
        """
        return np.array(image).astype(np.float32) / 255.0
    
    def process(self, image: Image.Image) -> Image.Image:
        """–ü–æ–ª–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.
        
        Args:
            image: –ò—Å—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        
        Returns:
            –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        """
        # 1. –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ RGB (–µ—Å–ª–∏ grayscale)
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        # 2. Resize
        image = self.resize_keep_aspect_ratio(image)
        
        return image
    
    def save(self, image: Image.Image, output_path: Path) -> None:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.
        
        Args:
            image: PIL Image
            output_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        """
        output_path.parent.mkdir(parents=True, exist_ok=True)
        image.save(output_path, 'JPEG', quality=self.jpeg_quality)
```

---

### 4. –ö–ª–∞—Å—Å `DatasetSplitter` (utils/dataset_splitter.py)

**–û—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å**: Split –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–∞ train/val

```python
from typing import List, Dict, Tuple
import random
import json
from pathlib import Path


class DatasetSplitter:
    """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–∞ train/val —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º.
    
    Attributes:
        train_ratio: –î–æ–ª—è train (0.0 - 1.0)
        random_seed: Seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
    """
    
    def __init__(self, train_ratio: float = 0.8, random_seed: int = 42):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è splitter.
        
        Args:
            train_ratio: –î–æ–ª—è train (–Ω–∞–ø—Ä–∏–º–µ—Ä, 0.8 = 80%)
            random_seed: Random seed
        
        Raises:
            ValueError: –ï—Å–ª–∏ train_ratio –Ω–µ –≤ [0, 1]
        """
        if not 0 < train_ratio < 1:
            raise ValueError(f"train_ratio must be in (0, 1), got {train_ratio}")
        
        self.train_ratio = train_ratio
        self.random_seed = random_seed
        random.seed(random_seed)
    
    def split(self, dataset: List[Dict[str, any]]) -> Tuple[List[Dict], List[Dict]]:
        """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–∞ train/val.
        
        Args:
            dataset: –°–ø–∏—Å–æ–∫ –æ–±—Ä–∞–∑—Ü–æ–≤
        
        Returns:
            Tuple (train_samples, val_samples)
        
        Raises:
            ValueError: –ï—Å–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–π
        """
        if len(dataset) < 2:
            raise ValueError(f"Dataset too small for split: {len(dataset)} samples")
        
        # Shuffle
        shuffled = dataset.copy()
        random.shuffle(shuffled)
        
        # Split
        train_size = int(len(shuffled) * self.train_ratio)
        train_samples = shuffled[:train_size]
        val_samples = shuffled[train_size:]
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞: —Ö–æ—Ç—è –±—ã 1 –æ–±—Ä–∞–∑–µ—Ü –≤ val
        if len(val_samples) == 0:
            val_samples = [train_samples.pop()]
        
        return train_samples, val_samples
    
    def save_metadata(self, samples: List[Dict[str, any]], output_path: Path) -> None:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ metadata.json.
        
        Args:
            samples: –°–ø–∏—Å–æ–∫ –æ–±—Ä–∞–∑—Ü–æ–≤
            output_path: –ü—É—Ç—å –∫ metadata.json
        """
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        metadata = []
        for idx, sample in enumerate(samples):
            metadata.append({
                'image_id': f"{idx+1:05d}",
                'image_path': f"images/{idx+1:05d}.jpg",
                'text': sample['text'],
                'original_filename': sample['filename'],
                'width': sample['width'],
                'height': sample['height']
            })
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
```

---

### 5. –ì–ª–∞–≤–Ω—ã–π —Å–∫—Ä–∏–ø—Ç `prepare_dataset.py` (scripts/prepare_dataset.py)

**–û—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å**: –û—Ä–∫–µ—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—Å–µ –∫–ª–∞—Å—Å—ã

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è fine-tuning DeepSeek-OCR

–ê–≤—Ç–æ—Ä: –ù–∏–∫–æ–ª–∞–π
–î–∞—Ç–∞: 08.01.2026
–ü—Ä–æ–µ–∫—Ç: DSOCR-HW
"""

import sys
from pathlib import Path
import argparse
from datetime import datetime

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ PYTHONPATH
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from utils.logger import setup_logger
from utils.data_loader import DatasetLoader
from utils.data_validator import DataValidator
from utils.image_processor import ImageProcessor
from utils.dataset_splitter import DatasetSplitter


class DatasetPreparer:
    """–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞.
    
    –û—Ä–∫–µ—Å—Ç—Ä–∏—Ä—É–µ—Ç:
    - DatasetLoader
    - DataValidator
    - ImageProcessor
    - DatasetSplitter
    """
    
    def __init__(self, 
                 raw_images_dir: Path,
                 annotations_path: Path,
                 output_dir: Path,
                 train_ratio: float = 0.8,
                 max_image_size: int = 1024,
                 random_seed: int = 42):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è preparer.
        
        Args:
            raw_images_dir: –ü—É—Ç—å –∫ data/raw/
            annotations_path: –ü—É—Ç—å –∫ annotations.csv
            output_dir: –ü—É—Ç—å –∫ data/processed/
            train_ratio: –î–æ–ª—è train
            max_image_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            random_seed: Random seed
        """
        self.raw_images_dir = raw_images_dir
        self.annotations_path = annotations_path
        self.output_dir = output_dir
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self.loader = DatasetLoader(annotations_path, raw_images_dir)
        self.validator = DataValidator()
        self.processor = ImageProcessor(max_size=max_image_size)
        self.splitter = DatasetSplitter(train_ratio=train_ratio, random_seed=random_seed)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞
        log_file = Path("logs") / f"prepare_dataset_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        self.logger = setup_logger("prepare_dataset", log_file)
    
    def prepare(self) -> None:
        """–ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞."""
        try:
            self.logger.info("=" * 80)
            self.logger.info("–ù–∞—á–∞–ª–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞")
            self.logger.info("=" * 80)
            
            # 1. –ó–∞–≥—Ä—É–∑–∫–∞
            self.logger.info("–®–∞–≥ 1: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
            dataset = self.loader.load_dataset()
            self.logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ –æ–±—Ä–∞–∑—Ü–æ–≤: {len(dataset)}")
            
            # 2. –í–∞–ª–∏–¥–∞—Ü–∏—è
            self.logger.info("–®–∞–≥ 2: –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞...")
            dataset = self.validator.validate_dataset(dataset)
            self.logger.info(f"–í–∞–ª–∏–¥–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤: {len(dataset)}")
            
            # 3. Split
            self.logger.info("–®–∞–≥ 3: –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val...")
            train_samples, val_samples = self.splitter.split(dataset)
            self.logger.info(f"Train: {len(train_samples)}, Val: {len(val_samples)}")
            
            # 4. –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ train
            self.logger.info("–®–∞–≥ 4: –û–±—Ä–∞–±–æ—Ç–∫–∞ train...")
            self._process_and_save(train_samples, self.output_dir / "train")
            
            # 5. –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ val
            self.logger.info("–®–∞–≥ 5: –û–±—Ä–∞–±–æ—Ç–∫–∞ val...")
            self._process_and_save(val_samples, self.output_dir / "val")
            
            self.logger.info("=" * 80)
            self.logger.info("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
            self.logger.info("=" * 80)
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}", exc_info=True)
            raise
    
    def _process_and_save(self, samples: List[Dict[str, any]], output_dir: Path) -> None:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—Ä–∞–∑—Ü–æ–≤.
        
        Args:
            samples: –°–ø–∏—Å–æ–∫ –æ–±—Ä–∞–∑—Ü–æ–≤
            output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (train –∏–ª–∏ val)
        """
        images_dir = output_dir / "images"
        images_dir.mkdir(parents=True, exist_ok=True)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        for idx, sample in enumerate(samples):
            # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
            processed_image = self.processor.process(sample['image'])
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
            output_path = images_dir / f"{idx+1:05d}.jpg"
            self.processor.save(processed_image, output_path)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            sample['processed_width'], sample['processed_height'] = processed_image.size
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ metadata.json
        metadata_path = output_dir / "metadata.json"
        self.splitter.save_metadata(samples, metadata_path)
        
        self.logger.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ {output_dir}: {len(samples)} –æ–±—Ä–∞–∑—Ü–æ–≤")


def main(args):
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    preparer = DatasetPreparer(
        raw_images_dir=Path(args.raw_images),
        annotations_path=Path(args.annotations),
        output_dir=Path(args.output),
        train_ratio=args.train_ratio,
        max_image_size=args.max_size,
        random_seed=args.seed
    )
    
    preparer.prepare()
    return 0


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è fine-tuning")
    parser.add_argument("--raw-images", type=str, default="data/raw",
                        help="–ü—É—Ç—å –∫ —Å—ã—Ä—ã–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º")
    parser.add_argument("--annotations", type=str, default="data/annotated/annotations.csv",
                        help="–ü—É—Ç—å –∫ CSV —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏")
    parser.add_argument("--output", type=str, default="data/processed",
                        help="–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
    parser.add_argument("--train-ratio", type=float, default=0.8,
                        help="–î–æ–ª—è train (0.0 - 1.0)")
    parser.add_argument("--max-size", type=int, default=1024,
                        help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
    parser.add_argument("--seed", type=int, default=42,
                        help="Random seed")
    
    args = parser.parse_args()
    sys.exit(main(args))
```

---

## üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

### –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç (2 –æ–±—Ä–∞–∑—Ü–∞)

**–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞**:
1. –ü–æ–ª–æ–∂–∏—Ç—å 2 —Å–∫–∞–Ω–∞ –≤ `data/raw/`: `test_001.jpg`, `test_002.jpg`
2. –°–æ–∑–¥–∞—Ç—å `data/annotated/annotations.csv`:
```csv
filename,text
test_001.jpg,"–¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç 1"
test_002.jpg,"–¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç 2"
```

**–ó–∞–ø—É—Å–∫**:
```powershell
python scripts/prepare_dataset.py --raw-images data/raw --annotations data/annotated/annotations.csv --output data/processed
```

**–û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç**:
```
data/processed/
‚îú‚îÄ‚îÄ train/
‚îÇ   ‚îú‚îÄ‚îÄ images/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 00001.jpg
‚îÇ   ‚îî‚îÄ‚îÄ metadata.json
‚îî‚îÄ‚îÄ val/
    ‚îú‚îÄ‚îÄ images/
    ‚îÇ   ‚îî‚îÄ‚îÄ 00001.jpg
    ‚îî‚îÄ‚îÄ metadata.json
```

### Unit-—Ç–µ—Å—Ç—ã (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –¥–ª—è –±—É–¥—É—â–µ–≥–æ)

```python
# tests/test_data_loader.py
import pytest
from utils.data_loader import DatasetLoader

def test_load_annotations():
    loader = DatasetLoader(
        annotations_path=Path("data/test/annotations.csv"),
        images_dir=Path("data/test/images")
    )
    df = loader.load_annotations()
    assert len(df) > 0
    assert 'filename' in df.columns
    assert 'text' in df.columns
```

---

## üìù –ß–µ–∫–ª–∏—Å—Ç –ø–µ—Ä–µ–¥ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ–º Issue

- [ ] –í—Å–µ –∫–ª–∞—Å—Å—ã —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω—ã –≤ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –º–æ–¥—É–ª—è—Ö (`utils/*.py`)
- [ ] –í—Å–µ –º–µ—Ç–æ–¥—ã –∏–º–µ—é—Ç type hints
- [ ] –í—Å–µ –∫–ª–∞—Å—Å—ã –∏ –º–µ—Ç–æ–¥—ã –∏–º–µ—é—Ç docstrings (Google style)
- [ ] –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ —á–µ—Ä–µ–∑ `utils/logger.py`
- [ ] –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ (try-except) –¥–ª—è –≤—Å–µ—Ö –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
- [ ] –ö–æ–¥ –ø—Ä–æ–≤–µ—Ä–µ–Ω –Ω–∞ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å: `python -m py_compile scripts/prepare_dataset.py`
- [ ] –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ (2 –æ–±—Ä–∞–∑—Ü–∞)
- [ ] –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ `data/processed/train/` –∏ `data/processed/val/`
- [ ] –õ–æ–≥–∏ —Å–æ–∑–¥–∞–Ω—ã –≤ `logs/prepare_dataset_*.log`
- [ ] Git –∫–æ–º–º–∏—Ç: `git commit -m "feat: implement prepare_dataset.py with OOP architecture (Issue #1)"`
- [ ] Pull Request —Å–æ–∑–¥–∞–Ω ‚Üí –∂–¥—ë–º —Ä–µ–≤—å—é –æ—Ç –°–µ–º—ë–Ω–∞

---

## üöÄ –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏ –ø–æ—Å–ª–µ Issue #1

1. **Issue #2**: `train_lora.py` ‚Äî –∑–∞–≥—Ä—É–∑–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏ fine-tuning
2. **Issue #3**: `evaluate.py` ‚Äî —Ä–∞—Å—á—ë—Ç –º–µ—Ç—Ä–∏–∫ CER/WER
3. **Issue #4**: `inference.py` ‚Äî –∏–Ω—Ñ–µ—Ä–µ–Ω—Å —Å LoRA-–∞–¥–∞–ø—Ç–µ—Ä–æ–º

---

## üìû –ö–æ–Ω—Ç–∞–∫—Ç—ã

**–í–æ–ø—Ä–æ—Å—ã –ø–æ –∑–∞–¥–∞—á–µ**: —Å–æ–∑–¥–∞–π Issue —Å –º–µ—Ç–∫–æ–π `question` –∏ —Ç–µ–≥–æ–º `@Tech Lead`  
**Code review**: Pull Request ‚Üí @–°–µ–º—ë–Ω  
**–ü—Ä–æ–±–ª–µ–º—ã —Å –¥–∞–Ω–Ω—ã–º–∏**: Issue —Å —Ç–µ–≥–æ–º `@Product Owner`

---

**–£–¥–∞—á–∏, –ù–∏–∫–æ–ª–∞–π! –ñ–¥—ë–º —Ç–≤–æ–π Pull Request! üöÄ**
